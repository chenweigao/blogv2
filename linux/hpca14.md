# HPCA 14

æœ¬ç« èŠ‚ç ”ç©¶æ–‡ç«  *Practical Data Value Speculation for Future High-end Processors*[^1], ç®€ç§° HPCA 14, è¿™ç¯‡æ–‡ç« ä¸»è¦æ˜¯ç ”ç©¶ CVP, ä¸€ç§ä¸Šä¸‹æ–‡æœ‰å…³çš„ã€Load value çš„é¢„æµ‹å™¨ã€‚



## Abstract

> Dedicating more silicon area to single thread performance will necessarily be considered as worthwhile in future â€“ potentially heterogeneous â€“ multicores. 
>
> In particular, Value prediction (VP) was proposed in the mid 90â€™s to enhance the performance of high-end uniprocessors by breaking true data dependencies.

ä½œè€…è¯´åœ¨æœªæ¥çš„å¤šæ ¸æ¶æ„ä¸­ï¼Œå°†æ›´å¤šçš„ç¡…é¢ç§¯ç”¨äºæé«˜å•çº¿ç¨‹çš„æ€§èƒ½æ˜¯å€¼å¾—çš„ã€‚

ç‰¹åˆ«æ˜¯ VP çš„å‡ºç°ï¼Œå¯ä»¥æ‰“ç ´çœŸæ­£çš„æ•°æ®ä¾èµ–æ¥æé«˜é«˜ç«¯å•å¤„ç†å™¨çš„æ€§èƒ½ã€‚

:::tip ğŸ’šğŸ’š æµæ°´çº¿å¹¶è¡Œ & å¤„ç†å™¨å¹¶è¡Œ

- æµæ°´çº¿ï¼šæé«˜æŒ‡ä»¤çš„å¹¶è¡Œåº¦ï¼›æµæ°´çº¿èšç„¦äºæŒ‡ä»¤ï¼Œæ‰€ä»¥æ˜¯ uniprocessor
- å¤šå¤„ç†å™¨ï¼šæé«˜å¤„ç†å™¨çš„å¹¶è¡Œåº¦

:::

> In this paper, we reconsider the concept of Value Prediction in the contemporary context and show its potential as a direction to improve current single thread performance.
>
> First, building on top of research carried out during the previous decade on confidence estimation, we show that every value predictor is amenable to very high prediction accuracy using very simple hardware. This clears the path to an implementation of VP without a complex selective reissue mechanism to absorb mispredictions. 
>
> Prediction is performed in the in-order pipeline frond-end and validation is performed in the in-order pipeline back-end, while the outof-order engine is only marginally modified.

ä½œè€…åœ¨å½“ä»£è¯­å¢ƒ(contemporary context) ä¸‹é‡æ–°æ€è€ƒäº† VP çš„æ¦‚å¿µï¼Œå¹¶ä¸”å‘è§‰å…¶ä½œä¸ºæé«˜å•çº¿ç¨‹æ€§èƒ½æ–¹å‘çš„ä¸€ä¸ªæ½œåŠ›ã€‚

é¦–å…ˆä½œè€…é˜è¿°äº†ç®€å•çš„ç¡¬ä»¶å°±å¯ä»¥å®ç°ç²¾ç¡®åº¦è¾ƒé«˜çš„ VP, ä¹Ÿä¸ç”¨å¾ˆå¤æ‚çš„ selective reissue æœºåˆ¶ã€‚

:::tip å•çº¿ç¨‹æµæ°´çº¿ vs å¤šçº¿ç¨‹æµæ°´çº¿

@todo ğŸ”´ğŸ”´ğŸ”´ 

:::

> Second, when predicting **back-to-back occurrences** of the same instruction, previous context-based value predictors relying on local value history exhibit a complex critical loop that should ideally be implemented in a single cycle. 
>
> To bypass this requirement, we introduce a new value predictor VTAGE *harnessing the global branch history*. VTAGE can seamlessly predict back-to-back occurrences, allowing predictions to *span over several cycles*. It achieves higher performance than previously proposed context-based predictors.

å…¶æ¬¡ï¼Œå¯¹äºåŒä¸€ä¸ªæŒ‡ä»¤çš„ back-to-back å‡ºç°ï¼Œä»¥å‰åŸºç¡€ local value history çš„æ–¹æ³•ä¼šæ˜¾ç¤ºå‡ºä¸€ä¸ªå¤æ‚çš„å…³é”®å¾ªç¯ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½œè€…å¼•å…¥äº†ä¸€ä¸ªæ–°çš„é¢„æµ‹å™¨ VTAG, åˆ©ç”¨å…¨å±€åˆ†æ”¯å†å²ï¼ŒVTAG å¯ä»¥æ— ç¼é¢„æµ‹ back-to-back çš„å‘ç”Ÿï¼Œå…¶å…è®¸é¢„æµ‹è·¨è¶Šå‡ ä¸ªå‘¨æœŸã€‚ç›¸æ¯”äºä»¥å‰çš„åŸºäºä¸Šä¸‹æ–‡çš„é¢„æµ‹å™¨ï¼Œå®ç°äº†æ›´é«˜çš„æ€§èƒ½ã€‚

:::warning ğŸ§¡ğŸ§¡ ä¸€äº›ç†è§£

1. VTAGE åˆ©ç”¨äº†å…¨å±€åˆ†æ”¯å†å²ï¼Œæ˜¯å¦‚ä½•ä½“ç°çš„ï¼Ÿ
2. span over several cycles, å¦‚ä½•è·¨è¶Šå‡ ä¸ª cycle?

:::

## Introduction

> Gabbay et al. and Lipasti et al. independently proposed Value Prediction to speculatively ignore true data dependencies and therefore shorten critical paths in computations. 

VP å¯ä»¥ç¼©çŸ­å…³é”®è·¯å¾„ã€‚

> Said penalty can be as high as the cost of a branch misprediction, yet the benefit of an individual correct prediction is often very limited.

é”™è¯¯æƒ©ç½šå¯èƒ½å’Œåˆ†æ”¯é¢„æµ‹çš„é”™è¯¯æƒ©ç½šä¸€æ ·é«˜ï¼Œæ‰€ä»¥å•ä¸ªæ­£ç¡®é¢„æµ‹çš„æ”¶ç›Šååˆ†æœ‰é™ã€‚

> As a consequence, high coverage is mostly irrelevant in the presence of low accuracy.

åœ¨ç²¾åº¦æä½çš„æƒ…å†µä¸‹ï¼Œé«˜è¦†ç›–ç‡åè€Œæ˜¯æ²¡æœ‰å¿…è¦çš„ã€‚

åŸºäºä»¥ä¸Šä¸¤æ®µè¯ï¼Œé¢„æµ‹çš„è®¾è®¡æ€è·¯åœ¨äºï¼šæé«˜é¢„æµ‹çš„å‡†ç¡®ç‡ï¼Œå¯ä»¥æ¥å—é€‚å½“é™ä½è¦†ç›–ç‡ï¼›æ•…æ­¤ä½œè€…æå‡º FPC, å…¶å®šä¹‰å¦‚ä¸‹ï¼š

> The Forward Probabilistic Counters (FPC) scheme yields value misprediction rates well under 1%, at the cost of reasonably decreasing predictor coverage.

FPC çš„é”™è¯¯é¢„æµ‹ç‡è¿œä½äº 1%ï¼ŒåŒæ—¶ç‰ºç‰²äº†é¢„æµ‹è¦†ç›–ç‡ã€‚

ä½¿ç”¨ FPC çš„å¥½å¤„å¦‚ä¸‹ï¼š

> Our experiments show that when FPC is used, no complex repair mechanism such as selective reissue is needed at execution time.

ä½¿ç”¨ FPC çš„è¯å¯ä»¥é¿å…ä½¿ç”¨å¦‚ selective reissue è¿™ç§å¤æ‚æœºåˆ¶ã€‚



æœ¬æ–‡çš„è´¡çŒ®ä¸»è¦ç”±ä¸¤ç‚¹ï¼š

> First, we present a simple yet efficient confidence estimation mechanism for value predictors. The Forward Probabilistic Counters (FPC) scheme yields value misprediction rates well under 1%, at the cost of reasonably decreasing predictor coverage. 
>
> All classical predictors are amenable to this level of accuracy. 
>
> FPC is very *simple to implement* and does not require substantial change in the counters update automaton. Our experiments show that when FPC is used, no complex repair mechanism such as selective reissue  is needed at execution time. Prediction validation can even be delayed until commit time and be done in-order: Complex and power hungry logic needed for execution time validation is not required anymore. As a result, prediction is performed in the in-order pipeline front-end, validation is performed in the in-order pipeline back-end while the out-of-order execution engine is only marginally modified.

ç¬¬ä¸€ç‚¹æ˜¯æå‡ºäº† FPC, ä¸€ä¸ªæ–°çš„è®¡æ•°å™¨ã€‚

- å®ç°ç®€å•ï¼Œä¸éœ€è¦å¯¹è®¡æ•°å™¨æ›´æ–°è‡ªåŠ¨æœºè¿›è¡Œå®è´¨æ€§æ›´æ”¹
- åœ¨æ‰§è¡Œé˜¶æ®µä¸éœ€è¦ä½¿ç”¨å¤æ‚çš„ä¿®å¤æœºåˆ¶å¦‚ selective reissue
- Validation å¯ä»¥æ¨è¿Ÿåˆ° commit é˜¶æ®µå¹¶æŒ‰é¡ºåºå®Œæˆï¼›è¿™æ„å‘³ç€å¤æ‚è€—ç”µçš„ execution é˜¶æ®µçš„ valudation ä¸å†éœ€è¦äº†
- out-of-order engine åªéœ€åšè½»å¾®ä¿®æ”¹

:::tip éšæƒ³

FPC æ˜¯ä¸€ç§ç½®ä¿¡åº¦çš„è¡¡é‡æœºåˆ¶ã€‚FPC çš„ä½œç”¨åœ¨äºé™ä½ misprediction rate.

:::

> Second, we introduce the Value TAGE predictor (VTAGE). This predictor is directly derived from research propositions on branch predictors [21] and more precisely from the indirect branch predictor ITTAGE. 
>
> VTAGE is the first hardware value predictor to leverage a long global branch history and the path history. Like all other value predictors, VTAGE is amenable to very high accuracy thanks to the FPC scheme. 
>
> VTAGE is shown to outperform previously proposed context-based predictors such as Finite Context Method and complements stride-based predictors.

ç¬¬äºŒç‚¹æ˜¯æå‡ºäº† VTAGE é¢„æµ‹å™¨ã€‚

- VTAGE æ˜¯ä¸€ä¸ªç¡¬ä»¶ value predictor. å…¶åˆ©ç”¨äº†é•¿æœŸå…¨å±€ branch history å’Œ path history.
- ç”±äº FPC æœºåˆ¶ï¼ŒVTAGE å…·æœ‰å¾ˆé«˜çš„ç²¾åº¦

:::tip éšæƒ³

ä¸Šè¿°è¿™æ®µè¯å®šä¹‰äº† VTAG, å…¶åŸºæœ¬å±æ€§æ˜¯å€¼é¢„æµ‹å™¨ï¼Œä½†æ˜¯åˆ©ç”¨äº†ï¼š

- global branch history
- path history

:::

> Moreover, we point out that unlike two-level predictors (in particular, predictors based on local value histories), VTAGE can seamlessly predict back-to-back occurrences of instructions, that is, instructions inside tight loops. Practical implementations are then feasible.

æ›´åŠ å‰å®³çš„æ˜¯ï¼Œä¸ä¸¤çº§é¢„æµ‹å™¨ï¼Œç‰¹åˆ«æ˜¯åŸºäº local value history çš„é¢„æµ‹å™¨ä¸åŒçš„æ˜¯ï¼ŒVTAGE å¯ä»¥å®Œç¾é¢„æµ‹æŒ‡ä»¤ back-to-back çš„å‡ºç°ï¼Œå³ tight loops.



ä¸‹é¢è¿™æ®µè¯æ¯”è¾ƒéš¾ä»¥ç†è§£ï¼š

> Prediction validation can even be delayed until commit time and be done in-order: Complex and power hungry logic needed for execution time validation is not required anymore.

é¢„æµ‹çš„éªŒè¯å¯ä»¥åœ¨ commit é˜¶æ®µå®Œæˆï¼Ÿæ‰€ä»¥è¯´ç®€åŒ–äº†éªŒè¯çš„æ­¥éª¤ã€‚

:::warning

âŒâŒâŒ ä½†æ˜¯è¿™æ ·çš„è¯ï¼Œæˆ‘ä»¬å¦‚ä½•ä¿è¯é¢„æµ‹çš„æ­£ç¡®æ€§å‘¢ï¼Ÿ

:::

ç»“åˆä¸‹é¢è¿™æ®µè¯ï¼Œçœ‹èƒ½å¦å°è¯•ç†è§£ï¼š

> As a result, prediction is performed in the in-order pipeline front-end, validation is performed in the in-order pipeline back-end while the out-of-order execution engine is only marginally modified.

ä¸Šè¿°ä¹Ÿæ˜¯åŸæ–‡ä¸­çš„æ‘˜å½•ã€‚

## 	Questions

ğŸ¤·â€â™‚ï¸ğŸ¤·â€â™‚ï¸ğŸ¤·â€â™‚ï¸ ä»ä»¥ä¸Šå¯¹äºæ–‡ç« çš„é˜…è¯»ï¼Œæˆ‘ä»¬éœ€è¦ä»æ–‡ç« ä¸­æ‰¾åˆ°ä»¥ä¸‹é—®é¢˜çš„ç­”æ¡ˆï¼š

1. FPC çš„å®ç°åŸç†æ˜¯ä»€ä¹ˆï¼Ÿ
2. VTAGE å¦‚ä½•åˆ©ç”¨ global branch history å’Œ path history? å…¶ä¸ä¸Šä¸‹æ–‡æœ‰å…³æ˜¯å¦‚ä½•ä½“ç°çš„ï¼Ÿ
3. VATGE å¦‚ä½•è§£å†³ tight lopp çš„é—®é¢˜ï¼Ÿ

## Related Work on VP

æˆ‘ä»¬æœ‰å¿…è¦ç ”ç©¶ä¸€ä¸‹ç›¸å…³çš„å·¥ä½œï¼Œçœ‹èƒ½å¦ä»ä¸­è·å¾—ä¸€äº›å¿ƒå¾—ä½“ä¼šã€‚

> Sazeides et al. refine the taxonomy of Value Prediction by categorizing predictors.

ä¸Šè¿°ä½œè€…å°† predictors åˆ†æˆäº†ä¸¤ç±»ï¼š

1. Computationalï¼Œè®¡ç®—çš„
2. Context-based

```mermaid
flowchart LR
	Predictors --- Computational
	Predictors --- Context-based
	Computational --- 2d(2-D Stride Predictor)
	Context-based --- VATGE
	Context-based --- FCM
	FCM --- Diff-FCM
	FCM --- gDiff
```



è¿™ä¸¤ç§æ–¹å¼æ˜¯äº’è¡¥çš„å› ä¸ºå®ƒä»¬æ“…é•¿é¢„æµ‹ä¸åŒçš„æŒ‡ä»¤ï¼ˆå‰æ–‡ç ”ç©¶çš„ HPCA19 çš„æ–‡ç« ä¹Ÿæ˜¯ä½¿ç”¨äº† 4 ä¸ªé¢„æµ‹å™¨ï¼ŒæŒ–æ˜å‡ºæ¥äº†äº’è¡¥çš„å…³ç³»ï¼‰ã€‚

å¯¹äº Computational é¢„æµ‹å™¨è€Œè¨€ï¼ŒğŸŸ¢ğŸŸ¢ğŸŸ¢ å…¸å‹çš„å¦‚ 2-Delta Stride predictor è¿™ç§éœ€è¦è¿›è¡Œç ”ç©¶ã€‚å¯¹äº Computational é¢„æµ‹å™¨è€Œè¨€ï¼Œå…¶é€šè¿‡åº”ç”¨ä¸€ä¸ª fucntion å»é¢„æµ‹ values.

å¯¹äº Context-based çš„é¢„æµ‹å™¨è€Œè¨€ï¼Œå°±æ˜¯æ ¹æ®é¢„æµ‹çš„å†å²æ¥å®ç°å€¼çš„é¢„æµ‹ï¼Œå…¸å‹çš„å¦‚ $n^th$ order *Finite Context Method(FCM)*  é¢„æµ‹å™¨ï¼Œè¿™äº›é¢„æµ‹å™¨ä¸€èˆ¬ä½¿ç”¨ two-level çš„é¢„æµ‹ç»“æ„ï¼š

1. ç¬¬ä¸€å±‚ç»“æ„æ˜¯ VHT(Value History Table) 
2. ç¬¬äºŒå±‚ç»“æ„æ˜¯ VPT(Value Prediction Table)

VHT hash åˆ° VPT ä¸Šï¼ŒVPT ä¸ŠåŒ…å«äº†å®é™…çš„é¢„æµ‹ã€‚éœ€è¦æ³¨æ„ï¼Œé€šå¸¸è€Œè¨€ï¼ŒVHT å’Œ VPT éƒ½å«æœ‰ä¸€ä¸ªé¥±å’Œè®¡æ•°å™¨ï¼Œä»¥ä¾¿äºè¡¡é‡ç½®ä¿¡åº¦ã€‚

Goeman å®ç°äº† diff-FCM, é€šè¿‡è¿½è¸ª local history ä¸­çš„ diff, è€Œä¸æ˜¯ value æœ¬èº«ï¼Œè¿™æ ·è¾¾åˆ°äº†æ›´åŠ èŠ‚çœç©ºé—´çš„ç›®çš„ã€‚

Zhou å®ç°äº† gDiff é¢„æµ‹å™¨ï¼ŒgDiff è®¡ç®—äº†ä¸€ä¸ªæŒ‡ä»¤çš„ç»“æœå’Œæœ€å n ä¸ªåŠ¨æ€æŒ‡ä»¤ç»“æœä¹‹é—´çš„ diff, å¦‚æœå‘ç°äº†ä¸€ä¸ª stable difference, ä¹Ÿå¯ä»¥ç§°ä¹‹ä¸º stride, åˆ™è¯¥æŒ‡ä»¤çš„ç»“æœå¯ä»¥é€šè¿‡å…ˆå‰é¢æŒ‡ä»¤è¿›è¡Œé¢„æµ‹ã€‚ç„¶è€Œï¼ŒgDiff çš„ç¼ºé™·åœ¨äºï¼Œå…¶ä¾èµ–äºå¦ä¸€ä¸ªé¢„æµ‹å™¨åœ¨é¢„æµ‹é˜¶æ®µå»é¢„æµ‹å…¨å±€çš„æŠ•æœºå€¼ã€‚ä½†æ˜¯æ­£å› ä¸ºå¦‚æ­¤ï¼ŒgDiff é¢„æµ‹å™¨å¯ä»¥è¢«æ·»åŠ åœ¨ä»»ä½• top of any predictor.

æœ¬æ–‡æå‡ºæ¥çš„ VTAGE é¢„æµ‹å™¨å¯ä»¥ç†è§£ä¸ºä¸€ä¸ª context-based çš„é¢„æµ‹å™¨ï¼Œå…¶ä¸­çš„ context åŒ…æ‹¬ global branch history å’Œ path history.

## Motivation

> We identify two factors that will complicate the adaptation and implementation of value predictors in future processor cores. 
>
> First, *the misprediction recovery penalty and/or hardware complexity*. Second *the back-to-back predictions for two occurrences of the same instruction which can be very complex to implement while being required to predict tight loops.*

æœ‰ä¸¤ä¸ªå› ç´ å¯èƒ½ä½¿å¾—é¢„æµ‹å™¨å¤æ‚åŒ–ï¼š

1. Misprediction Recovery
2. Back-to-back prediction

> A tight loop is a loop that loops many times and the loop body has few instructions.

### Misprediction Recovery

ä¹‹å‰çš„å¾ˆå¤šç ”ç©¶éƒ½æ²¡æœ‰æ„è¯†åˆ° misprediction recovery çš„å¤æ‚æ€§ï¼Œè€Œåªå…³æ³¨äºå‡†ç¡®ç‡æˆ–è€…è¦†ç›–ç‡ï¼Œå¿½ç•¥äº†å®é™…çš„åŠ é€Ÿæ•ˆæœã€‚åç»­çš„å¾ˆå¤šç ”ç©¶ä¹ŸåŸºæœ¬ä¸Šå¿½ç•¥äº†ä¸ misprediction recovery  ç›¸å…³çš„æ€§èƒ½æŸå¤±ã€‚

> Moreover, despite quite high coverage and reasonable accuracy, one observation that can be made from these early studies is that the average performance gain per correct prediction is rather small.

ä¸Šè¿°è¯è¯´æ˜äº†ï¼Œå•ä¸ªæ­£ç¡®é¢„æµ‹çš„æ”¶ç›Šæ¯”è¾ƒæœ‰é™ã€‚

è¡¡é‡ misprediction çš„ recovery çš„æ¶ˆè€—å¯ä»¥åˆ†ä¸ºä¸¤ä¸ªå› ç´ ï¼š

1. average misprediction penalty(å¤„ç½š) $P_{value}$
2. absolute number of misprediction $N_{misp}$

äºæ˜¯æœ‰æ€»çš„é”™è¯¯é¢„æµ‹æƒ©ç½šè®¡ç®—å¦‚ä¸‹ï¼š
$$
T_{recov} = P_{value} * N_{misp}
$$

> As we already pointed out, the total misprediction recovery cost can be minimized through two vehicles: Minimizing the individual misprediction penalty and/or minimizing the total number of mispredictions.

ä»ä¸Šè¿°å…¬å¼ä¸­æˆ‘ä»¬å¯ä»¥å¾—å‡ºç»“è®ºï¼Œé™ä½ cost å¯ä»¥ä½¿ç”¨ä¸¤ç§æ–¹å¼ï¼š

- é™ä½å•ä¸ªé¢„æµ‹é”™è¯¯æƒ©ç½š
- æœ€å°åŒ–é”™è¯¯é¢„æµ‹æ•°é‡



#### Value Misprediction Scenarios

å¤„ç†å™¨ä¸­ç›®å‰å·²æœ‰ä¸¤ç§æœºåˆ¶å»ç®¡ç† value misprediciton recovery:

1. pipline squashing
2. selective reissue

ä¸åŒä¹‹å¤„å¦‚ä¸‹ï¼š

> They induce very different average misprediction penalties, but are also very different from a hardware complexity standpoint.

è¿™ä¸¤è€…äº§ç”Ÿçš„å¹³å‡é”™è¯¯é¢„æµ‹æƒ©ç½šä¸åŒï¼Œç¡¬ä»¶å¤æ‚æ€§ä¹Ÿä¸åŒã€‚

ğŸ’šğŸ’š ä»€ä¹ˆæ˜¯ pipline squashing?

æš‚æ—¶å¯ä»¥ç†è§£ä¸º pipline flushing, clearing or squashing.

ğŸ’šğŸ’š pipline squashing åšäº†ä»€ä¹ˆäº‹æƒ…ï¼Ÿ

ç›®å‰çŒœæµ‹çš„ï¼Œéœ€è¦ç»§ç»­ç ”ç©¶ã€‚

> Pipeline squashing is already implemented to recover from branch mispredictions. *On a branch misprediction, all the subsequent instructions in the pipeline are flushed* and instruction fetch is resumed at the branch target. This mechanism is also generally used on load/store dependency mispredictions.
> Using pipeline squashing on a value misprediction is *straightforward*, but *costly as the minimum misprediction penalty* is the same as the minimum branch misprediction penalty. However, to limit the number of squashes due to VP,  **squashing can be avoided if the predicted result has not been used yet, that is, if no dependent instruction has been issued.**

pipline squashing å¯ä»¥è¢«ç”¨äºåˆ†æ”¯é¢„æµ‹å¤±è´¥çš„ recovery ä¸­ï¼Œä¹Ÿå¯ä»¥ç”¨ä¸ VP å¤±è´¥çš„ recovery ä¸­ï¼Œä¸¤è€…çš„ä»£ä»·æ˜¯ä¸€è‡´çš„ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼ŒVP çš„ squash å¯ä»¥è¢«é¿å…çš„ï¼Œåªè¦é¢„æµ‹çš„ç»“æœè¿˜æ²¡æœ‰è¢«åº”ç”¨ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œæ²¡æœ‰ dependent instruction è¢« issued.

âŒâŒ selective reissue ä¸æ˜¯å¾ˆå¥½ç†è§£ï¼Œéœ€è¦å†ç†è§£ä¸€ä¸‹ã€‚

> Selective reissue is implemented in processors to recover in case where *instructions have been executed with incorrect operands*, in particular this is used to recover from L1 cache hit/miss mispredictions (i.e. load-dependent instructions are issued after predicting a L1 hit, but finally the load results in a L1 miss). When the execution of an instruction with an incorrect operand is detected, the instruction as well as all its dependent chain of instructions are canceled then replayed.



#### Validation at Execution vs Validation at Commit Time

ä¸‹é¢æ˜¯å¯¹äºä¸¤ç§æœºåˆ¶çš„å¯¹æ¯”ï¼š

1. åœ¨å®ç°çš„èŠ‚ç‚¹ä¸Šï¼Œselective issue å¿…é¡»æ˜¯åœ¨ execution çš„æ—¶å€™å°±å®ç°äº†ï¼Œå…¶ç›®çš„æ˜¯ä¸ºäº†é™åˆ¶é”™è¯¯é¢„æµ‹çš„ä»£ä»·ï¼›è€Œ pipeline squashing åˆ™å¯ä»¥åœ¨ execution æˆ–è€… commit çš„æ—¶å€™å®ç°ã€‚
2. pipeline squashing åœ¨ execution æ—¶é—´å» validate é¢„æµ‹å¿…é¡»é‡æ–°è®¾è®¡ out-of-order engine, é™¤æ­¤ä¹‹å¤–ï¼Œé¢„æµ‹çš„ value è¿˜å¿…é¡»åœ¨æ¯ä¸ªä¹±åºçš„é˜¶æ®µä¼ æ’­ï¼Œç­‰ç­‰ã€‚ç»¼åˆæ¥çœ‹ï¼Œåœ¨ exec é˜¶æ®µå»éªŒè¯æ¯”è¾ƒå¤æ‚ã€‚
3. åä¹‹ï¼Œåœ¨ commit åè¿›è¡Œ pipeline squashing å¯èƒ½ä¼šå¯¼è‡´é¢„æµ‹é”™è¯¯åä»£ä»·è¾ƒé«˜ï¼Œä½†æ˜¯å…¶å®ç°æœºåˆ¶è¾ƒä¸ºç®€å•ï¼Œç‰¹åˆ«æ˜¯å¯¹äº out-of-order æ¥è¯´ï¼Œä¸éœ€è¦å¢åŠ é¢å¤–çš„å¤æ‚æœºåˆ¶ã€‚

ç®€å•ä½¿ç”¨è¡¨æ ¼è¿›è¡Œæ¦‚æ‹¬ï¼š

|                    | Validation at Execution                                      | Validation at Commit                                         |
| ------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| Pipeline Squashing | - Results in a minimum misprediction penalty <br />- Need to redesign the complete out-of-order engine<br />- 20 ~ 40 cycle penalty | - Results in a quite high average misprediction penalty <br />- Do not reduce complex mechanisms in the out-of-order execution engine<br />- 40 ~ 50 cycle penalty |
| Selective Reissue  | - Must be implemented at execution time<br />- 5 ~ 7 cycle penalty | N/A                                                          |

å¦‚æœé€‰æ‹© validation at execution çš„è¯ï¼š

> However, validating predictions *at execution time* necessitates to *redesign the complete out-of-order engine*: The predicted values must be propagated through all the out-of-execution engine stages and the predicted results must be validated as soon as they are produced in this out-of- order execution engine.

éœ€è¦é‡æ–°è®¾è®¡ä¹±åºå¼•æ“ï¼Œé¢„æµ‹çš„å€¼éœ€è¦åœ¨æ‰€æœ‰çš„ stage ä¼ æ’­å¹¶ä¸”é¢„æµ‹çš„ç»“æœå¿…é¡»ç»è¿‡éªŒè¯ã€‚

> On the contrary, **pipeline squashing at commit** results in a quite high average misprediction penalty since it can delay prediction validation by a substantial number of cycles. Yet, it is much easier to implement for Value Prediction since it does not induce complex mechanisms in the out-of-order execution engine. 
>
> It essentially restrains the Value Prediction related hardware to the in-order pipeline front-end (prediction) and the in-order pipeline back-end (validation and training). Moreover, it allows not to checkpoint the rename table since the committed rename map contains all the necessary mappings to restart execution in a correct fashion.

pipeline at commit ä¼šå¯¼è‡´è¾ƒé«˜çš„ misprediction penalty, ä½†æ˜¯å…¶ä¼˜ç‚¹åœ¨äºä¸éœ€è¦é‡æ–°è®¾è®¡å¤æ‚çš„ out-of-order engine.



**Balancing Accuracy and Coverage**

> The total misprediction penalty Trecov is roughly proportional to the number of mispredictions. Thus, if one drastically improves the accuracy at the cost of some coverage then, as long as the coverage of the predictor remains quite high, there might be a performance benefit brought by Value Prediction, even though the average value misprediction penalty is very high.

$T_{recov}$ ä¸é”™è¯¯é¢„æµ‹çš„æ•°é‡å¤§è‡´æˆæ­£æ¯”ï¼Œæ‰€ä»¥å¦‚æœå¯ä»¥åœ¨ç‰ºç‰²ä¸€äº›è¦†ç›–ç‡çš„æƒ…å†µä¸‹æå‡ç²¾åº¦ï¼Œé‚£ä¹ˆæ€»çš„ VP æ€§èƒ½æ˜¯å¯ä»¥å¾—åˆ°æå‡çš„ã€‚



### Back-to-back prediction

> Unlike a branch prediction, a value prediction is needed rather late in the pipeline (at dispatch time).

ä¸åŒäºåˆ†æ”¯é¢„æµ‹ï¼Œå€¼é¢„æµ‹åœ¨ pipeline ä¸­è¢«éœ€è¦çš„ç›¸å½“æ™šï¼ˆåœ¨ dispatch é˜¶æ®µï¼‰

:::tip ğŸ“ğŸ“ğŸ“ dispatch

> The instruction dispatch unit controls when the decoded instructions are dispatched to the execution pipelines. It includes **Issue Queues** for storing instruction pending dispatch to execution pipelines[^2].

ä»ä¸Šé¢çš„æè¿°æˆ‘ä»¬å¯ä»¥çœ‹å‡ºï¼Œdispatch é˜¶æ®µå¤„äºæŒ‡ä»¤ decode ä¹‹åï¼Œexecution ä¹‹å‰ã€‚

åœ¨æŸäº›æ¶æ„ä¸­ï¼Œå°±æ˜¯ issue.

 Issue Queues æ˜¯ç”¨æ¥ä¿å­˜å°†è¦è¢« execution çš„æŒ‡ä»¤ã€‚

:::

> Thus, at first glance, prediction latency does not seem to be a concern and long lookups in large tables and/or fairly complex computations could be tolerated. 

åŸºäºä¸Šè¿°æˆ‘ä»¬åˆ†æçš„ VP è¢«éœ€è¦çš„é˜¶æ®µï¼Œä¹ä¸€çœ‹ï¼Œé¢„æµ‹çš„å»¶è¿Ÿä¼¼ä¹ä¸æ˜¯ä¸€ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¯ä»¥å®¹å¿å¤§è¡¨æˆ–è€…å¤æ‚è®¡ç®—ã€‚

> However, for most predictors, **the outcomes of a few previous occurrences of the instruction are needed to perform a prediction for the current instance.**

ä½†æ˜¯ï¼Œå¯¹äºå¤§å¤šæ•°çš„é¢„æµ‹å™¨è€Œè¨€ï¼Œå½“å‰å®ä¾‹çš„é¢„æµ‹æ˜¯ä¾èµ–äºæŒ‡ä»¤å…ˆå‰å‡ºç°æŒ‡ä»¤çš„å‡ æ¬¡ç»“æœçš„ã€‚

ğŸ’šğŸ’š è¿™è¾¹æœ‰ä¸ªç»†èŠ‚ï¼Œä»”ç»†çœ‹é‚£æ®µè‹±æ–‡åŸæ–‡çš„è¯æˆ‘ä»¬å¯ä»¥å‘ç°ï¼Œå…¶æè¿°çš„ä¸»é¢˜å¯¹è±¡ä¸€ç›´æ˜¯ instruction, å³æŒ‡ä»¤å…ˆå‰çš„å‡ºç°å’Œå½“å‰é¢„æµ‹ä¹‹é—´çš„å…³ç³»ã€‚

> Consequently, for those predictors, either the critical operation must be made short enough to allow for the prediction of close (possibly back-to-back) occurrences (e.g. by using small tables) or the prediction of tight loops must be given up.

æ‰€ä»¥ï¼Œå¯¹è¿™äº›é¢„æµ‹å™¨è€Œè¨€ï¼Œå°±è¦æ±‚å…³é”®è·¯å¾„å°½å¯èƒ½çš„çŸ­ã€‚æˆ–è€…è¯´ tight loop å¿…é¡»å°½å¯èƒ½å¾—æ”¾å¼ƒã€‚

> Unfortunately, tight loops with candidates for VP are quite abundant in existing programs.

ä¸å¹¸çš„æ˜¯ï¼Œtight loop è¿™ç§æƒ…å†µåœ¨ç¨‹åºä¸­å¾ˆå¤šã€‚

> Experiments conducted with the methodology we will introduce in Section 7 suggest that for a subset of the SPECâ€™00/â€™06 benchmark suites, there can be as much as 15.3% (3.4% a-mean) fetched instructions eligible for VP and for which **the previous occurrence was fetched in the previous cycle** (8-wide Fetch). We highlight such critical operations for each predictor in the subsequent paragraphs.

ä¸Šè¿°æ–‡å­—ä¸»è¦æè¿°äº†å®éªŒç»“æœï¼Œé‡è¦çš„éƒ¨åˆ† highlight å‡ºæ¥ã€‚



*æ€»ç»“ä¸€ä¸‹ï¼Œæœ¬éƒ¨åˆ†é¦–å…ˆé˜è¿°äº† VP éœ€è¦å€¼çš„é˜¶æ®µè¾ƒä¸ºé åï¼Œæ‰€ä»¥æ˜¯å…è®¸é¢„æµ‹å»¶è¿Ÿçš„ï¼Œä½†æ˜¯å¯¹äº tight loop ç±»ä¼¼çš„åœºæ™¯ï¼Œä¼šè¦æ±‚å…³é”®è·¯å¾„å°½å¯èƒ½çŸ­ï¼ˆæˆ–è€…è¯´å»¶è¿Ÿå°½å¯èƒ½å°ï¼‰ã€‚ä½œè€…åœ¨åé¢é€šè¿‡å®éªŒçš„ç»“æœé˜è¿°äº† tight loop åœºæ™¯åœ¨å®é™…æ˜¯æ™®éå­˜åœ¨çš„ã€‚*

æ¥ä¸‹æ¥ä¸»è¦æ˜¯å¯¹æ¯” LVP, stride å’Œ FCM, åˆ†åˆ«é˜è¿°è¿™å‡ ä¸ªé¢„æµ‹å™¨çš„ä¼˜ç¼ºç‚¹ã€‚

#### LVP

> Despite its name, LVP does not require the previous prediction to predict the current instance as long as the table is trained. Consequently, LVP uses only the program counter to generate a prediction.

LVP ä¸éœ€è¦ä¾èµ–å…ˆå‰çš„é¢„æµ‹ç»“æœï¼Œä½†æ˜¯å…¶ä¾èµ–äºç¨‹åºè®¡æ•°å™¨ PC çš„ç»“æœã€‚

> Thus, successive table lookups are independent and can last until **Dispatch**, meaning that large tables can be implemented.

å› æ­¤ï¼Œè¿ç»­çš„è¡¨æŸ¥æ‰¾æ˜¯ç‹¬ç«‹çš„ï¼Œå¯ä»¥æŒç»­åˆ° dispatch é˜¶æ®µï¼Œå› æ­¤ LVP æ˜¯å¯ä»¥ä½¿ç”¨å¤§è¡¨çš„ã€‚

#### Stride

@todo

#### FCM

å…¨ç§°æ˜¯ Finite Context Method, å…¶ç»“æ„æ˜¯ two-level:

> The first-level consists of a value history table accessed using the instruction address. This history is then hashed and used to index the second level table.

è¿™ä¸ªä¸¤çº§ç»“æ„çš„å›¾å¯ä»¥å‚è€ƒ Figure 1.



#### Summary

ä¸Šé¢é˜è¿°äº†ä¸‰ä¸ªé¢„æµ‹å™¨çš„å®ç°ç»†èŠ‚å’Œç¼ºç‚¹ã€‚

> Table lookup time is not an issue as long as the prediction arrives before Dispatch for LVP and Stride. Therefore, large predictor tables can be considered for implementation. For stride-based value predictor, the main difficulty is that one has to track the last (possibly speculative) occurrence of each instruction.

ä¸Šè¿°æ–‡å­—è¯´æ˜äº† LVP å’Œ stride çš„ç¼ºç‚¹ã€‚

> For local value based predictors the same difficulty arises with the addition of tracking the n last occurrences. Moreover the critical operations (hash and the 2nd level table read) lead to either using small tables or not being able to timely predict back-to-back occurrences of the same instruction. Implementations of such predictors can only be justified if they bring significant performance benefit over alternative predictors.

åŸºäº local value çš„é¢„æµ‹å™¨ä¹Ÿä¼šå—åˆ°å…³é”®æ“ä½œï¼ˆhash å’Œç¬¬ 2 çº§è¡¨çš„è¯»å–ï¼‰çš„å½±å“ã€‚

> The VTAGE predictor we introduce in this paper is able to seamlessly predict back-to-back occurrences of the same instruction, thus its access can span over several cycles. VTAGE does not require any complex tracking of the last occurrences of the instruction. 
>
> Section 8 shows that VTAGE (resp. hybrid predictor using VTAGE) outperforms a local value based FCM predictor (resp. hybrid predictor using a local value based FCM predictor).

æœ¬æ–‡æå‡ºçš„ VTAGE é¢„æµ‹å™¨å¯ä»¥å®Œç¾é¢„æµ‹ back-to-back åœºæ™¯ï¼Œå› æ­¤å®ƒçš„è®¿é—®å¯ä»¥è·¨è¶Šå‡ ä¸ªå¾ªç¯ã€‚

### Commit Time Validation and Hardware Implications on the Out-of-Order Engine

> In the previous section, we have pointed out that the hardware modifications induced by *pipeline squashing* at *commit time on* the Out-of-Order engine are limited. 
>
> In practice, the only major modification compared with a processor without Value Prediction is that the predicted values must be written in the physical registers before Dispatch.

å‰é¢çš„ç« èŠ‚æåˆ°äº†ï¼Œpipline squashing + commit time validation å¯¹ out-of-order çš„å½±å“æ˜¯æœ‰é™çš„ã€‚

äº‹å®ä¸Šï¼Œåœ¨ä¸æ²¡æœ‰ VP çš„å¤„ç†å™¨ç›¸æ¯”ï¼Œå”¯ä¸€çš„ä¸»è¦ä¿®æ”¹æ—¶ï¼šé¢„æµ‹å€¼å¿…é¡»åœ¨ dispatch ä¹‹å‰å†™å…¥ç‰©ç†å¯„å­˜å™¨ã€‚

> At first glance, if every destination register has to be predicted for each fetch group, one would conclude that the number of write ports should double. 
>
> In that case the overhead on the register file would be quite high. The area cost of a register file is approximately proportional to $(R + W) *  (R + 2W)$, $R$ and $W$ respectively being the number of read ports and the number of write ports.
>
> Assuming $R = 2W$, the area cost without VP would be proportional to $12W^2$ and the one with VP would be proportional to $24W^2$, i.e. the double. Energy consumed in the register file would also be increased by around 50% (using very simple Cacti 5.3 approximation).

ä¹ä¸€çœ‹ï¼Œå¦‚æœæ¯ä¸€ä¸ª fetch group çš„ç›®æ ‡å¯„å­˜å™¨éƒ½å¿…é¡»è¢«é¢„æµ‹çš„è¯ï¼Œåˆ™ä¼šå¾—å‡ºç»“è®ºï¼Œwrite ports åº”è¯¥ç¿»ä¸€ç•ªã€‚

æ¥ä¸‹æ¥æ˜¯å¯¹å¯„å­˜å™¨æ¶ˆè€—çš„è®¡ç®—ï¼Œå…¶ä¸­ $R$ è¡¨ç¤ºè¯»å¯„å­˜å™¨ï¼Œ$W$ è¡¨ç¤ºå†™å¯„å­˜å™¨ã€‚

ç»è¿‡ä¸€ç³»åˆ—çš„è®¡ç®—ï¼Œæœ€åå‘ç°å¯„å­˜å™¨çš„é¢ç§¯è¦å¢åŠ  50%!

> For practical implementations, there exist several opportunities to limit this overhead. 
>
> *For instance one can limit the number of extra ports needed to write predictions.* Each cycle, only a few predictions are used and the predictions can be known several cycles before Dispatch: One could limit the number of writes on each cycle to a certain limit, and buffer the extra writes, if there are any. 
>
> Assuming only $W/2$ write ports for writing predicted values leads to a register file area of $35W^2 /2$ , saving half of the overhead of the naive solution. The same saving on energy is observed (Cacti 5.3 estimations).
>
> Another opportunity is to *allocate physical registers for consecutive instructions in different register file banks, limiting the number of write ports on the individual banks.* One can also prioritize the predictions according to the criticality of the instruction and only use the most critical one, leveraging the work on criticality estimation of Fields et.

ä½†æ˜¯äº‹å®ä¸Šï¼Œæœ‰ä¸€äº› opportunities å»é™åˆ¶è¿™ä¸€å¼€é”€ã€‚

1. ä¾‹å¦‚å¯ä»¥é™åˆ¶å†™å…¥é¢„æµ‹éœ€è¦çš„é¢å¤–å¯„å­˜å™¨æ•°é‡ï¼Œä¸‹é¢æ˜¯ä¸€äº›ä¸¾ä¾‹å’Œè®¡ç®—ã€‚
2. ä¸ºè¿ç»­æŒ‡ä»¤åˆ†é…ç‰©ç†å¯„å­˜å™¨ï¼Œé™åˆ¶å•ä¸ª bank çš„å†™å…¥ç«¯å£æ•°ã€‚ç”šè‡³ä¹Ÿå¯ä»¥æ ¹æ®ä¼˜å…ˆçº§é€‰æ‹©ä½¿ç”¨æœ€å…³é”®çš„æŒ‡ä»¤ã€‚

> Exploring the complete optimization to reduce the overhead on the register file design is out of the scope of this paper. It would depend on the precise micro-architecture of the processor, but we have clearly shown that this overhead in terms of energy and silicon area can be reduced to less than 25% and 50% respectively. Moreover, this overhead is restricted to the register file and does not impact the other components of the out-of-order engine. Similarly, thanks to commit time validation, the power overhead introduced by Value Prediction will essentially reside in the predictor table.

ä½œè€…è¯´æ˜äº†å‡å°‘å¯„å­˜å™¨çš„æ•°é‡ä¸åœ¨æœ¬æ–‡çš„ç ”ç©¶èŒƒå›´ä¹‹å†…ã€‚

âŒâŒâŒ è¿™å¥è¯éš¾ç†è§£ï¼šSimilarly, thanks to commit time validation, the power overhead introduced by Value Prediction will essentially reside in the predictor table.

### Maximizing Value Predictor Accuracy Through Confidence

> As we already pointed out, the total misprediction recovery cost can be minimized through two vehicles: **Minimizing the *individual misprediction penalty* and/or minimizing the *total number of mispredictions.***

é”™è¯¯é¢„æµ‹çš„æ¢å¤æŸè€—ä»ä¸¤ä¸ªæ–¹é¢è¡¡é‡ã€‚

> When using the prediction is not mandatory (i.e. contrarily to branch predictions), an efficient way to minimize the number of mispredictions is to use saturating counter to estimate confidence and use the prediction only when the associated confidence is very high. 
>
> For instance, for the value predictors considered in this study, a 3-bit confidence counter per entry that is reset on each misprediction leads to an accuracy in the 95-99% range if the prediction is used only when the counter is saturated. However this level of accuracy is still not sufficient to avoid performance loss in several cases unless idealistic selective reissue is used. 
>
> To increase accuracy, Burtscher et al. proposed the SAg confidence  stimation scheme to assign confidence to a history of outcomes rather than to a particular instruction. However, this entails a second lookup in the counter table using the outcome history retrieved in the predictor table with the PC of the instruction. A way to maximize accuracy without increasing complexity and latency would be preferable.

å½“é¢„æµ‹ä¸æ˜¯å¼ºåˆ¶çš„æ—¶å€™ï¼Œä½¿ç”¨é¥±å’Œè®¡æ•°å™¨ï¼Œæœ€å°åŒ–é”™è¯¯é¢„æµ‹çš„æ•°é‡ï¼Œè®¡ç®—ç½®ä¿¡å€¼å¹¶ä¸”åªä½¿ç”¨ç½®ä¿¡åº¦å¾ˆé«˜çš„é¢„æµ‹ã€‚ä¸¾ä¾‹äº† 3-bit é¥±å’Œè®¡æ•°å™¨çš„åˆç†ä½¿ç”¨å¯ä»¥è¾¾åˆ° 95% - 99% çš„å‡†ç¡®ç‡ï¼Œä½†æ˜¯è¿™ä¸ªå‡†ç¡®ç‡è¿˜æ˜¯ä¸å¤Ÿï¼Œæœ‰äº›ä¸“å®¶æå‡ºäº† SAg ç½®ä¿¡åº¦ä¼°è®¡æ–¹æ¡ˆï¼Œä½†æ˜¯ä¼šå¢åŠ å¤æ‚æ€§ã€‚ç°åœ¨éœ€è¦ä¸€ä¸ªå‡†ç¡®åº¦é«˜çš„ï¼Œä½†æ˜¯ä¸å¢åŠ å¤æ‚æ€§å’Œæ—¶å»¶çš„æ–¹æ³•ã€‚

> We actually found that simply using **wider counters** (e.g. 6 or 7 bits) leads to much more accurate predictors while the prediction coverage is only reduced by a fraction. 
>
> Prediction is only used on saturated confidence counters and counters are reset on each misprediction. Interestingly, probabilistic 3-bit counters such as defined by Riley et al.  augmented with reset on misprediction achieve the same accuracy for substantially less storage and a marginal increase in complexity.

ä½œè€…å‘ç°ä½¿ç”¨ wider counters å¯ä»¥æå‡å¾ˆå¤šçš„é¢„æµ‹å‡†ç¡®åº¦ï¼Œéšä¹‹çš„ä»£ä»·æ˜¯å¾ˆå°çš„è¦†ç›–ç‡æŸå¤±ã€‚

*è¿™å¥è¯çš„æ„æ€æ˜¯è¯´ï¼Œä½¿ç”¨æ›´å¤š bit ä½çš„è®¡æ•°å™¨ï¼Œå…¶é¢„æµ‹ç²¾åº¦ä¼šæé«˜ï¼›ä½†æ˜¯ä½œè€…è¯´æ˜çš„ï¼Œè¦†ç›–ç‡ä¼šé™ä½ï¼Œæˆ‘çŒœæµ‹å¯èƒ½æ˜¯å› ä¸ºä½¿ç”¨äº†é¥±å’Œè®¡æ•°å™¨çš„ç¼˜æ•…ï¼Œç›®å‰é¢„æµ‹ä»…åœ¨é¥±å’Œè®¡æ•°å™¨é¥±å’Œçš„æ—¶å€™è¿›è¡Œé¢„æµ‹ï¼Œé‚£ä¹ˆå°±æ„å‘³ç€ï¼Œè¶Šæ™šé¥±å’Œï¼Œé‚£ä¹ˆé¢„æµ‹çš„è¦†ç›–ç‡å°±è¶Šä½ã€‚*

è¿™ä¸ªä¼˜ç‚¹å¾ˆå¤šï¼Œå…·ä½“æ€ä¹ˆä½¿ç”¨ï¼Œè¦åœ¨åæ–‡ç ”ç©¶ã€‚

> We refer to these probabilistic counters as Forward Probabilistic Counters (FPC). **In particular, each forward transition is only triggered with a certain probability.** 
>
> In this paper, we will consider 3-bit confidence counters using a probability vector $v = \{1, 1/16, 1/16, 1/16, 1/16, 1/32, 1/32\}$ for pipeline squashing at commit and $v = \{1, 1/8, 1/8, 1/8, 1/8, 1/16, 1/16\}$ for selective reissue, respectively mimicking 7-bit and 6-bit counters.
>
> This generally prevents all the considered VP schemes to slow down execution while minimizing the loss of coverage (as opposed to using lower probabilities). The used pseudo-random generator is a simple Linear Feedback Shift Register.

ä½¿ç”¨äº† FPC è®¡æ•°å™¨ï¼Œå¹¶ä¸”æå‡ºäº† 3-bit è®¡æ•°å™¨ï¼Œæ¯ä¸€ä½å­˜åœ¨ä¸€ä¸ªæŒ‡å®šçš„æ¦‚ç‡å€¼ã€‚

è¿™ä¸ª FPC ç¿»è¯‘åç§°å«åšå‰å‘æ¦‚ç‡è®¡æ•°å™¨ï¼Œæ˜¯ä»¥å›ºå®šçš„æ¦‚ç‡è§¦å‘çš„ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œè¿™ç§è®¡æ•°å™¨çš„ä¼˜ç‚¹åœ¨äºï¼Œæˆ‘åªæ˜¯ä½¿ç”¨äº† 3-bit, å°±è¾¾åˆ°äº† 6-bit æˆ–è€… 7-bit çš„æ•ˆæœã€‚ç”±æ­¤ï¼Œè¾¾åˆ°äº†æˆ‘ä»¬ä¸Šæ–‡æåˆ°çš„ï¼Œå‡†ç¡®ç‡é«˜ä½†æ˜¯ä¸å¢åŠ å¤æ‚æ€§å’Œæ—¶å»¶ã€‚

æ³¨æ„åœ¨ HPCA 19 çš„æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†è¿™ä¸ª FPC.

> Using FPC counters instead of full counters limits the overhead of confidence estimation. It also opens the opportunity to adapt the probabilities at run-time as suggested in and/or to individualize these probabilities depending on the criticality of the instructions.

ä½¿ç”¨ FPC è®¡æ•°å™¨è€Œä¸æ˜¯å®Œæ•´è®¡æ•°å™¨é™åˆ¶äº†ç½®ä¿¡åº¦ä¼°è®¡çš„å¼€é”€ï¼Œå¹¶ä¸”è¿˜æä¾›äº†åœ¨è¿è¡Œæ—¶è°ƒæ•´æ¦‚ç‡çš„æœºä¼šï¼Œå¦‚æ ¹æ®é‡è¦æŒ‡ä»¤ä¸ªæ€§åŒ–æ¦‚ç‡ã€‚

### The Value TAgged GEometric Predictor

é¢˜ç›®çš„å«ä¹‰ä¸ºï¼šå€¼æ ‡è®°çš„å‡ ä½•é¢„æµ‹å™¨ã€‚

ç¬¬ä¸€æ®µé¦–å…ˆåˆ—ä¸¾å‡ºæ¥äº† VTAGE æ¥æºäºåˆ†æ”¯é¢„æµ‹çš„ ITTAGE, ITTAGE æ¥æºäº TAGE.

> As it uses branch history to predict, we expect VTAGE to perform much better than other predictors when instruction results are **indeed depending on the control flow**. 
>
> Nonetheless, VTAGE is also able to capture control-flow independent patterns as long as they are short enough with regard to the maximum history length used by the predictor. 
>
> In particular, it can still capture short strided patterns, although space efficiency is not optimal since each value of the pattern will reside in an entry (contrarily to the Stride predictor where one pattern can be represented by a single entry).

è¿™ä¸€æ®µçš„ç»†èŠ‚æˆ‘ä»¬æš‚æ—¶ä¸è¿›è¡Œè€ƒç©¶ã€‚

TAGE ä½¿ç”¨äº†åˆ†æ”¯çš„å†å²è¿›è¡Œé¢„æµ‹ï¼Œå½“æŒ‡ä»¤çš„ç»“æœå®é™…ä¾èµ–äºæ§åˆ¶æµçš„æ—¶å€™ï¼Œæˆ‘ä»¬å¸Œæœ› VTAGE èƒ½æ¯”å…¶ä»–çš„é¢„æµ‹å™¨è¡¨ç°å¾—æ›´å¥½ã€‚å°½ç®¡å¦‚æ­¤ï¼ŒVTAGE ä¹Ÿèƒ½å¤Ÿæ•è· control-flow independent patterns, åªè¦ä»–ä»¬ç›¸å¯¹äºé¢„æµ‹å™¨ä½¿ç”¨çš„æœ€å¤§å†å²é•¿åº¦è¶³å¤ŸçŸ­ã€‚

:::warning âŒâŒ æ§åˆ¶æµ

éœ€è¦ç†è§£æ–‡ç« ä¸­æ‰€è¯´çš„æ§åˆ¶æµæ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿåœ¨ä»€ä¹ˆæƒ…å†µä¸‹ï¼ŒæŒ‡ä»¤çš„ç»“æœæ˜¯å–å†³äºæ§åˆ¶æµçš„ï¼Ÿ

:::

> Fig. 2 describes a (1+N)-component VTAGE predictor. The main idea of the VTAGE scheme (exactly like the ITTAGE scheme) is to use several tables â€“ components â€“ storing predictions. Each table is indexed by a different number of bits of the global branch history, hashed with the PC of the instruction. 
>
> The different lengths form a geometric series (i.e. VT1 is accessed with two bits of the history, VT2 with four, VT3 with eight and so on). 
>
> These tables are backed up by a base predictor â€“ a tagless LVP predictor â€“ which is accessed using the instruction address only. 
>
> In VTAGE, an entry of a tagged component consists of a partial tag, a 1-bit usefulness counter u used by the replacement policy, a full 64-bit value val, and a confidence/hysteresis counter c. An entry of the base predictor simply consists of the prediction and the confidence counter.

ä¸Šè¿°æ–‡å­—åœ¨é™ˆè¿° VTAGE é¢„æµ‹å™¨æ˜¯å¦‚ä½•å®ç°çš„ï¼Œè¿™æ®µæ¯”è¾ƒé‡è¦ã€‚

å›¾ 2ï¼ˆæœ¬ç¯‡æ–‡ç« ä¸­æ²¡æœ‰ç»™å‡ºï¼‰æè¿°äº†ä¸€ä¸ª 1+N ç»„ä»¶çš„ VTAGE é¢„æµ‹å™¨ã€‚å…¶æ–¹æ¡ˆçš„æ ¸å¿ƒæ€æƒ³æ˜¯ä½¿ç”¨ä¸€äº›è¡¨ï¼Œä¹Ÿå¯ä»¥è¯´æ˜¯ç»„ä»¶ï¼Œå»å­˜å‚¨é¢„æµ‹ï¼Œæ¯ä¸€ä¸ªè¡¨éƒ½è¢«å…¨å±€åˆ†æ”¯å†å²çš„ bit æ•°é‡ç´¢å¼•ï¼Œè¢«æŒ‡ä»¤çš„ PC æ‰€ hash.

:::warning âŒâŒâŒ bits of global barnch history

è¿™é‡Œæåˆ°äº†å…¨å±€é¢„æµ‹å†å²çš„ bit æ•°é‡ï¼Œåœ¨æŸ¥é˜…èµ„æ–™ä»¥åï¼Œè¿™ä¸ªçš„æ„æ€å¯èƒ½æ˜¯ï¼Œåœ¨åˆ†æ”¯é¢„æµ‹ä¸­ï¼Œå­˜åœ¨ä¸€ä¸ªå…¨å±€åˆ†æ”¯é¢„æµ‹å†å²å¯„å­˜å™¨ Global History Register (GHR), è¿™ä¸ª GHR å¯èƒ½ç”± 10-bit ç»„æˆï¼Œå¯ä»¥ç”¨æ¥è¡¨ç¤ºæœ€è¿‘ 10 ä¸ªåˆ†æ”¯çš„å†å²ï¼Œè€Œè¿™ 10-bit å¯ä»¥ç”¨æ¥ç´¢å¼• 1024 ä¸ª PHT çš„ entry, æ¯ä¸€ä¸ª entry ç”± 2-bit ç»„æˆï¼Œæ˜¯ä¸€ä¸ªé¥±å’Œè®¡æ•°å™¨ï¼Œç´¢å¼•çš„æ–¹å¼æ˜¯ PC çš„å 10-bit ä¸ GHR è¿›è¡Œå¼‚æˆ–[^3]ã€‚

ğŸ”´ğŸ”´ è‡³äºä¸ºä»€ä¹ˆæ˜¯å¼‚æˆ–ï¼Œè¿˜éœ€è¦è¿›è¡Œæ·±å…¥çš„æ€è€ƒã€‚ 

:::

ä¸åŒçš„é•¿åº¦å½¢æˆä¸€ä¸ªå‡ ä½•çº§æ•°ã€‚

VTAGE ä¸»è¦æ˜¯ä½¿ç”¨äº†å¾ˆå¤š table, VT1, VT2, â€¦, VTn, åˆ†åˆ«ä»£è¡¨çš„å«ä¹‰æ˜¯ï¼šVT1 å…³è”äº† 2-bit çš„ global branch history, VT2 ä¸º 4-bit, VT3 ä¸º 8-bit, ä»¥æ­¤ç±»æ¨ï¼Œè¿™å°±æ˜¯ç­‰æ¯”çº§æ•°æˆ–è€…å‡ ä½•çº§æ•°ã€‚

è¿™äº›è¡¨ç”±æ— æ ‡è®° LVP é¢„æµ‹å™¨å¤‡ä»½ï¼Œè¯¥é¢„æµ‹å™¨ä»…ä»…ä½¿ç”¨æŒ‡ä»¤åœ°å€è®¿é—®ã€‚

ç¬¬ä¸‰æ®µè®²è¿°äº†é¢„æµ‹å™¨å…·ä½“çš„å®ç°ç»†èŠ‚ã€‚

> At **prediction time**, all components are searched in parallel to check for a tag match. The matching component accessed with the longest history is called the provider component as it will provide the prediction to the pipeline.

åœ¨é¢„æµ‹çš„æ—¶å€™ï¼Œå¹¶è¡ŒæŸ¥æ‰¾ä¸ tag åŒ¹é…çš„æ¡ç›® match çš„ç»„ä»¶å¹¶ä¸”ä¸ longest history è”ç³»çš„ç§°ä½œ provider component, åœ¨æµæ°´çº¿ä¸­æä¾›é¢„æµ‹ã€‚

è¿™è¾¹çš„ longest history çš„æ„æ€æ˜¯è¯´ï¼Œbit æ•°æœ€é•¿çš„ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œä»å¤§åˆ°å°è¿›è¡ŒæŸ¥æ‰¾ï¼›æ¯”å¦‚è¯´å¯¹äºä¸€ä¸ª Global History Register (GHR) è€Œè¨€ï¼Œå‡è®¾å…¶æœ‰ 10 ä½ï¼Œé‚£ä¹ˆæˆ‘çš„ VT1 æœ‰ 2 ä½ï¼ŒVT2 æ˜¯ 4 ä½â€¦å‡è®¾ VT2 å°±æ˜¯æœ€åä¸€ä¸ªï¼Œé‚£ä¹ˆæˆ‘å°±ä» VT2 å¼€å§‹æŸ¥æ‰¾ï¼Œè¿™å°±æ˜¯ longest history.

> At **update time**, only the provider is updated. 
>
> On either a correct or an incorrect prediction, $c$ and $u$ are updated.  
>
> On a misprediction, $val$ is replaced if $c$ is equal to 0, and a new entry is allocated in a component *using a longer history than the provider*: All â€œupperâ€ components are accessed to see if one of them has an entry that is not useful ($u$ is 0).  If none is found, the u counter of all matching entries in the upper components are reset, but no entry is allocated. Otherwise, a new entry is allocated in one of the components whose corresponding entry is not useful. The component is chosen randomly.

åœ¨æ›´æ–°çš„æ—¶å€™ï¼Œåªæ›´æ–° provider. (ä¹Ÿå°±æ˜¯è¯´ï¼Œåªæ›´æ–°æœ€é•¿å†å²çš„é‚£å¼ è¡¨)

æ— è®ºé¢„æµ‹æ˜¯æ­£ç¡®æˆ–è€…ä¸æ­£ç¡®ï¼Œ$c$ å’Œ $u$ ä¼šè¢«æ›´æ–°ã€‚å…¶ä¸­ c è¡¨ç¤ºé¥±å’Œè®¡æ•°å™¨ï¼Œu è¡¨ç¤ºæ˜¯å¦æœ‰ç”¨ï¼Œ1:useful, 0: not useful.

å¦‚æœæ˜¯ misprediction, $val$ ä¼šè¢«æ›¿æ¢æ‰ï¼Œå¦‚æœè®¡æ•°å™¨ c æ˜¯ 0 çš„è¯ï¼Œå¹¶ä¸”æ–°çš„æ¡ç›®ä¼šè¢«åˆ†é…ï¼Œä½¿ç”¨æ¯” provider æ›´é•¿çš„ history.  æ‰€æœ‰æ›´ä¸Šå±‚çš„ç»„ä»¶éƒ½è¢«è®¿é—®ï¼Œå»åˆ¤æ–­æ˜¯å¦å…¶ä¸­æœ‰ä¸€ä¸ª entry æ˜¯æ— ç”¨çš„ï¼Œåœ¨è¿™é‡Œ $u == 0$ï¼ˆäºŒè¿›åˆ¶ï¼‰æ˜¯æ— ç”¨çš„ï¼Œ$u$ æ˜¯ä¸€ä¸ª useful bit, å…¶è¢« replacement policy ä½¿ç”¨ã€‚

å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»»ä½•ä¸€ä¸ªï¼Œä¸Šå±‚ç»„ä»¶çš„ $u$ è®¡æ•°å™¨éƒ½è¢«é‡ç½®ï¼Œæ„å‘³ç€æ²¡æœ‰ entry è¢«åˆ†é…ã€‚å¦‚æœæ‰¾åˆ°äº†çš„è¯ï¼Œä¸€ä¸ªæ–°çš„ entry å°±è¢«åˆ†é…äº†ï¼Œè¢«åˆ†é…çš„ç­–ç•¥æ˜¯ï¼šéšæœºç­–ç•¥ï¼Œé€‰æ‹©ä¸€ä¸ªç»„ä»¶çš„ entry ä¸æ˜¯ useful çš„ã€‚

> The main difference between VTAGE and ITTAGE is essentially the usage: The predicted value is used only if its confidence counter is saturated. We refer the reader to for a detailed description of ITTAGE.

VTAGE å’Œ ITTAGE ä¸åŒçš„ç‚¹åœ¨äºï¼Œé¥±å’Œè®¡æ•°å™¨é¥±å’Œçš„æ—¶å€™æ‰ä½¿ç”¨é¢„æµ‹çš„å€¼ã€‚

> Lastly, as a prediction does not depend on previous values but only on previous control-flow, VTAGE can seamlessly predict instructions in tight loops and behaves like LVP in Fig. 1. However, due to index hash and multiplexing from multiple components, it is possible that its prediction latency will be higher, although this is unlikely to be an issue since prediction can span several cycles.

æœ€åï¼Œç”±äºé¢„æµ‹ä¸ä¾èµ–äºå…ˆå‰çš„å€¼ï¼Œè€Œåªä¾èµ–äºå…ˆå‰çš„æ§åˆ¶æµï¼ŒVTAGE å¯ä»¥å®Œç¾é¢„æµ‹ tight loop.

ç„¶è€Œï¼Œç”±äºå¤šä¸ªç»„ä»¶çš„ç´¢å¼•å“ˆå¸Œå’Œå¤ç”¨ï¼Œå…¶é¢„æµ‹å»¶è¿Ÿå¯èƒ½ä¼šæ›´é«˜ï¼Œå°½ç®¡è¿™å¯èƒ½ä¸æ˜¯ä¸€ä¸ªé—®é¢˜ï¼Œå› ä¸ºé¢„æµ‹æ˜¯å¯ä»¥è·¨å‘¨æœŸçš„ã€‚

## Evaluation Methodology

### Value Predictors

#### Single Scheme Predictors

> We study the behavior of several distinct value predictors in addition to VTAGE.
>
> Namely, LVP, the 2-delta Stride predictor (2D-Stride) as a representative of the stride-based predictor family4 and a generic order-4 FCM predictor (o4-FCM)

é™¤äº† VTAGE, æˆ‘ä»¬è¿˜éœ€è¦å¯¹æ¯”ä¸€äº› value predictors. åŒ…æ‹¬ LVP, 2D-Stride å’Œ o4-FCM.

> All predictors use 3-bit saturating counters as confidence counters. The prediction is used only if the confidence counter is saturated. 
>
> Baseline counters are incremented by one on a correct prediction and reset on a misprediction. The predictors were simulated with and without FPC (See Section 5). As the potential of VP has been covered extensively in previous work, we **limit ourselves to reasonably sized predictors** to gain more concrete insights. 
>
> We start from a 128KB LVP (8K-entry) and derive the other predictors, each of them having 8K entries as we wish to gauge the prediction generation method, not space efficiency. Predictor parameters are illustrated in Table 1.

å…ˆé˜è¿°è¿™äº›é¢„æµ‹å™¨éƒ½ä½¿ç”¨äº† 3-bit çš„é¥±å’Œè®¡æ•°å™¨ä½œä¸ºç½®ä¿¡åº¦çš„è¡¡é‡æ ‡å‡†ï¼Œå¹¶ä¸”åªæœ‰åœ¨é¥±å’Œè®¡æ•°å™¨é¥±å’Œçš„æ—¶å€™å¯¹åº”çš„é¢„æµ‹æ‰è¢«ä½¿ç”¨ã€‚

å¦‚æœé¢„æµ‹æˆåŠŸçš„è¯ï¼ŒåŸºçº¿çš„é¢„æµ‹å™¨ +1ï¼Œmisprediction çš„è¯å°±é‡ç½®ã€‚

é¢„æµ‹å™¨åœ¨æœ‰ FPC å’Œæ²¡æœ‰ FPC çš„æƒ…å†µä¸‹æ¨¡æ‹Ÿã€‚å¹¶ä¸”é™åˆ¶äº†é¢„æµ‹å™¨çš„å¤§å°ã€‚

âŒâŒ æˆ‘ä»¬ä» 128K çš„ LVP å¼€å§‹ï¼Œæ¨å¯¼å…¶ä»–é¢„æµ‹å™¨ï¼Œæ¯ä¸ªé¢„æµ‹å™¨éƒ½æœ‰ 8K ä¸ª entries, å› ä¸ºæˆ‘ä»¬å¸Œæœ›è¡¡é‡é¢„æµ‹ç”Ÿæˆæ–¹æ³•ï¼Œè€Œä¸æ˜¯ç©ºé—´æ•ˆç‡ã€‚

> For VTAGE, we consider a predictor featuring 6 tables in addition to a base component. The base component is a tagless LVP predictor. We use a single useful bit per entry in the tagged components and a 3-bit hysteresis/confidence counter c per entry in every component. The tag of tagged components is *12+rank-bit* long with *rank* varying between 1 and 6. The minimum and maximum history lengths are respectively 2 and 64 as we found that these values provided a good tradeoff in our experiments.

å¯¹äº VTAGE, æˆ‘ä»¬è€ƒè™‘ä¸€ä¸ªé¢„æµ‹å™¨ï¼Œé™¤äº†ä¸€ä¸ªåŸºç¡€ç»„ä»¶å¤–ï¼Œè¿˜åŒ…æ‹¬ 6 ä¸ªè¡¨ã€‚åŸºç¡€ç»„ä»¶æ˜¯ tagless çš„ LVP é¢„æµ‹å™¨ã€‚æˆ‘ä»¬åœ¨æ¯ä¸ª tagged çš„ç»„ä»¶ä¸­ä½¿ç”¨ä¸€ä¸ª useful æ ‡å¿—ä½å’Œä¸€ä¸ª 3-bit çš„ç½®ä¿¡åº¦/è¿Ÿæ»è®¡æ•°å™¨ã€‚tag å­—æ®µçš„å¤§å°æ˜¯ $12 + rank$ bit, $rank$ çš„å–å€¼åœ¨ 1~6 ä¹‹é—´ï¼Œç”±æ­¤è®¡ç®—ï¼Œæœ€å°å’Œæœ€å¤§çš„ history length çš„èŒƒå›´åœ¨ 2~64 ä¹‹é—´ï¼Œè¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„ trade-off.

> For o4-FCM, we use a hash function similar to thoseâ€¦..

è¯´äº†ä¸€ä¸‹ o4-FCM çš„ç»†èŠ‚ï¼Œæˆ‘ä»¬æš‚æ—¶ä¸å¯¹å…¶è¿›è¡Œç ”ç©¶ã€‚

> We consider that all predictors are able to predict instantaneously. **As a consequence, they can seamlessly deliver their prediction before Dispatch.** 
>
> This also implies that o4- CM is â€“ unrealistically â€“ able to deliver predictions for two occurrences of the same instruction fetched in two consecutive cycles. Hence, its performance is most likely to be overestimated.

æˆ‘ä»¬è®¤ä¸ºæ‰€æœ‰çš„é¢„æµ‹å™¨éƒ½å¯ä»¥ç¬é—´é¢„æµ‹ï¼Œå› æ­¤ï¼Œå®ƒä»¬å¯ä»¥åœ¨ dispatch ä¹‹å‰å®Œç¾åœ°ä¼ é€’é¢„æµ‹ã€‚

#### Hybrid Predictors

ä½œè€…é˜è¿°äº†ä¸€ä¸‹ï¼Œè¡¨æ˜æ··åˆé¢„æµ‹æ˜¯å¯è¡Œçš„ï¼ˆæ··åˆé¢„æµ‹æˆ‘ä»¬åœ¨ HPCA 19 ä¸­è¿›è¡Œé‡ç‚¹ç ”ç©¶ï¼‰ã€‚

### Simulator

>  In our experiments, we use the gem5 cycle-accurate simulator (x86 ISA).

å®éªŒä½¿ç”¨äº† gem5 ä»¿çœŸã€‚

> We model a fairly aggressive pipeline: 4GHz, 8-wide superscalar, out-of-order processor with a latency of 19 cycles. 
>
> We chose a slow front-end (15 cycles) coupled to a swift back-end (3 cycles) to obtain a realistic misprediction penalty. 

ä½œè€…æ¨¡æ‹Ÿäº†ç›¸å½“æ¿€è¿›çš„ pipline.

:::tip

ğŸ§¡ğŸ§¡ğŸ§¡

è¿™è¾¹éœ€è¦ä¸“é¢˜ç†è§£ï¼Œsuprescalar, latency çš„å…·ä½“å«ä¹‰ã€‚

ğŸ§¡ğŸ§¡ğŸ§¡

:::

ä½œè€…é€‰æ‹©äº†ä¸€ä¸ªæ…¢çš„å‰ç«¯è€¦åˆåˆ°å¿«é€Ÿçš„åç«¯ä¸­ï¼Œå¯ä»¥è§‚å¯Ÿé€¼çœŸçš„ misprediction æƒ©ç½šã€‚

#### Misprediction Recovery

> We illustrate two possible recovery scenarios, squashing at commit time and a very idealistic selective reissue.
>
> In both scenarios, recovery is unnecessary if the prediction of instruction I was wrong but no dependent instruction has been issued before the execution of I, since the prediction is replaced by the effective result at execution time. This removes useless squashes and is part of our implementation.

misprediction æ—¶å€™çš„æ¢å¤æœ‰ä¸¤ç§æ–¹å¼ï¼š

1. squashing at commit time
2. ååˆ†ç†æƒ³ä¸»ä¹‰çš„ selective reissue(ç†æƒ³ä¸»ä¹‰æ˜¯ä½œè€…å¯¹å…¶çš„è¯„ä»·ï¼Œä¸ä»£è¡¨æˆ‘æœ¬äººè§‚ç‚¹)

åœ¨ä¸Šè¿°ä¸¤ç§æƒ…å†µä¸‹ï¼Œå¦‚æœæŒ‡ä»¤çš„é¢„æµ‹é”™è¯¯ä½†æ˜¯å…¶åœ¨æ‰§è¡Œä¹‹å‰æ²¡æœ‰ issue ä¾èµ–æŒ‡ä»¤ï¼Œåˆ™ä¸éœ€è¦ recovery, å› ä¸ºé¢„æµ‹ä¼šè¢«æ‰§è¡Œæ—¶çš„æœ‰æ•ˆç»“æœå–ä»£ã€‚

## Reference

[^1]: A. Perais and A. Seznec, "Practical data value speculation for future high-end processors", *High Performance Computer Architecture (HPCA) 2014 IEEE 20th International Symposium on*, Feb 2014.

[^2]: [ARM Cortex-A75 Core Technical Reference Manual r2p0](https://developer.arm.com/documentation/100403/0200/functional-description/technical-overview/components/instruction-dispatch?lang=en)
[^3]: [Assignment 1: Understanding Branch Prediction](https://www.inf.ed.ac.uk/teaching/courses/car/Pracs/2017-18/Assignment1.pdf) 

