---
title: GPU Architecture Deep Dive
date: 2025-03-06
tags:
  - GPU
  - CUDA
  - Parallel Computing
  - AI Infrastructure
description: ç³»ç»Ÿæ€§è§£æç°ä»£GPUæ¶æ„è®¾è®¡åŸç†ï¼Œæ¶µç›–SIMTæ‰§è¡Œæ¨¡å‹ã€SMå¾®æ¶æ„ã€å†…å­˜å±‚æ¬¡ç»“æ„åŠçº¿ç¨‹è°ƒåº¦æœºåˆ¶
---

## 1. å¼•è¨€ï¼šGPU æ¶æ„è®¾è®¡å“²å­¦

### 1.1 CPU vs GPUï¼šè®¾è®¡ç›®æ ‡çš„æ ¹æœ¬å·®å¼‚

CPU å’Œ GPU çš„æ¶æ„å·®å¼‚æºäºå…¶è®¾è®¡ç›®æ ‡çš„æ ¹æœ¬ä¸åŒï¼š

```mermaid
graph TB
    subgraph CPU["ğŸ–¥ï¸ CPU: è¿½æ±‚ä½å»¶è¿Ÿ"]
        direction TB
        C_GOAL["ğŸ¯ ç›®æ ‡: å•ä»»åŠ¡æœ€å¿«å®Œæˆ"]
        C_GOAL --> C_CTRL["ğŸ§  å¤æ‚æ§åˆ¶é€»è¾‘"]
        C_GOAL --> C_CACHE["ğŸ’¾ å¤§å®¹é‡ç¼“å­˜ (MBçº§)"]
        C_GOAL --> C_BRANCH["ğŸ”® åˆ†æ”¯é¢„æµ‹å™¨"]
        C_GOAL --> C_OOO["âš¡ ä¹±åºæ‰§è¡Œ"]
        C_CTRL --> C_RESULT["å°‘é‡å¼ºæ ¸å¿ƒ<br/>4-64 cores"]
    end
    
    subgraph GPU["ğŸ® GPU: è¿½æ±‚é«˜åå"]
        direction TB
        G_GOAL["ğŸ¯ ç›®æ ‡: æ€»ä»»åŠ¡é‡æœ€å¤§"]
        G_GOAL --> G_CTRL["ğŸ“‹ ç®€å•æ§åˆ¶é€»è¾‘"]
        G_GOAL --> G_CACHE["ğŸ’¨ å°å®¹é‡æµå¼ç¼“å­˜"]
        G_GOAL --> G_SIMT["ğŸ”„ SIMT æ‰§è¡Œ"]
        G_GOAL --> G_HIDE["ğŸ­ å»¶è¿Ÿéšè—"]
        G_CTRL --> G_RESULT["å¤§é‡å¼±æ ¸å¿ƒ<br/>æ•°åƒ cores"]
    end
    
    style CPU fill:#4a90d9,stroke:#6ab0ff,stroke-width:2px,color:#fff
    style GPU fill:#d97b4a,stroke:#ffab6b,stroke-width:2px,color:#fff
    style C_GOAL fill:#3d7ab8,stroke:#5a9bd4,color:#fff
    style G_GOAL fill:#b8663d,stroke:#d4895a,color:#fff
    style C_RESULT fill:#2d5a8a,stroke:#4a90d9,stroke-width:2px,color:#fff
    style G_RESULT fill:#8a4a2d,stroke:#d97b4a,stroke-width:2px,color:#fff
```

| ç‰¹æ€§ | CPU | GPU |
|------|-----|-----|
| è®¾è®¡ç›®æ ‡ | æœ€å°åŒ–å•ä»»åŠ¡å»¶è¿Ÿ | æœ€å¤§åŒ–æ€»ä½“ååé‡ |
| æ ¸å¿ƒæ•°é‡ | å°‘é‡å¤æ‚æ ¸å¿ƒï¼ˆ4-64ï¼‰ | å¤§é‡ç®€å•æ ¸å¿ƒï¼ˆæ•°åƒï¼‰ |
| ç¼“å­˜ç­–ç•¥ | å¤§å®¹é‡å¤šçº§ç¼“å­˜ | å°å®¹é‡æµå¼ç¼“å­˜ |
| çº¿ç¨‹ç®¡ç† | æ“ä½œç³»ç»Ÿè½¯ä»¶è°ƒåº¦ | ç¡¬ä»¶è‡ªåŠ¨è°ƒåº¦ |
| åˆ†æ”¯å¤„ç† | å¤æ‚åˆ†æ”¯é¢„æµ‹å™¨ | è°“è¯æ‰§è¡Œ/åˆ†æ”¯åˆ†åŒ– |
| é€‚ç”¨åœºæ™¯ | å¤æ‚é€»è¾‘ã€ä½å»¶è¿Ÿä»»åŠ¡ | æ•°æ®å¹¶è¡Œã€é«˜ååä»»åŠ¡ |

### 1.2 GPU å¹¶è¡Œæ€§çš„å››ä¸ªå±‚æ¬¡

GPU é€šè¿‡å¤šå±‚æ¬¡å¹¶è¡Œå®ç°é«˜ååé‡è®¡ç®—ï¼š

```mermaid
graph TB
    subgraph "å¹¶è¡Œå±‚æ¬¡"
        L1["â‘  æŒ‡ä»¤çº§å¹¶è¡Œ (ILP)<br/>æµæ°´çº¿ã€æŒ‡ä»¤è°ƒåº¦"]
        L2["â‘¡ æ•°æ®çº§å¹¶è¡Œ (DLP)<br/>SIMD/SIMT æ‰§è¡Œ"]
        L3["â‘¢ çº¿ç¨‹çº§å¹¶è¡Œ (TLP)<br/>Warp è°ƒåº¦ã€å»¶è¿Ÿéšè—"]
        L4["â‘£ ä»»åŠ¡çº§å¹¶è¡Œ<br/>å¤š Stream å¹¶å‘"]
    end
    L1 --> L2 --> L3 --> L4
```

---

## 2. SIMT æ‰§è¡Œæ¨¡å‹ï¼šGPU çš„æ ¸å¿ƒèŒƒå¼

### 2.1 SIMT å®šä¹‰ä¸ç‰¹å¾

**SIMTï¼ˆSingle Instruction, Multiple Threadsï¼‰** æ˜¯ NVIDIA å®šä¹‰çš„ GPU æ‰§è¡Œæ¨¡å‹ï¼ŒåŒºåˆ«äºä¼ ç»Ÿ SIMDï¼š

```mermaid
graph TB
    subgraph SIMD["SIMD (AVX-512, NEON)"]
        direction LR
        S_IN["å‘é‡å¯„å­˜å™¨<br/>v[0:7]"] --> S_OP["å•æ¡å‘é‡æŒ‡ä»¤<br/>VADD v1,v2,v3"]
        S_OP --> S_OUT["å›ºå®šå®½åº¦è¾“å‡º<br/>ç¼–è¯‘æ—¶ç¡®å®š"]
    end
    
    subgraph SIMT["SIMT (CUDA, ROCm)"]
        direction LR
        T_IN["32 æ ‡é‡çº¿ç¨‹<br/>å„æœ‰ç‹¬ç«‹ PC/å¯„å­˜å™¨"] --> T_OP["åŒä¸€æŒ‡ä»¤å¹¿æ’­<br/>ç¡¬ä»¶è‡ªåŠ¨å‘é‡åŒ–"]
        T_OP --> T_OUT["çµæ´»åˆ†æ”¯å¤„ç†<br/>è¿è¡Œæ—¶æ©ç "]
    end
    
    SIMD -.->|"ç¨‹åºå‘˜ç®¡ç†å‘é‡"| SIMT
    SIMT -.->|"ç¡¬ä»¶ç®¡ç†å¹¶è¡Œ"| SIMD
    
    style SIMD fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
    style SIMT fill:#fff3e0,stroke:#f57c00,stroke-width:2px
```


| ç‰¹æ€§ | SIMD | SIMT |
|------|------|------|
| ç¼–ç¨‹æ¨¡å‹ | æ˜¾å¼å‘é‡æ“ä½œ | æ ‡é‡çº¿ç¨‹ç¼–ç¨‹ |
| å‘é‡å®½åº¦ | ç¼–è¯‘æ—¶å›ºå®š | è¿è¡Œæ—¶ç”±ç¡¬ä»¶ç®¡ç† |
| åˆ†æ”¯å¤„ç† | éœ€è¦æ˜¾å¼æ©ç  | ç¡¬ä»¶è‡ªåŠ¨å¤„ç†åˆ†åŒ– |
| çº¿ç¨‹ç‹¬ç«‹æ€§ | æ— ç‹¬ç«‹çŠ¶æ€ | æ¯çº¿ç¨‹ç‹¬ç«‹å¯„å­˜å™¨/PC |
| å…¸å‹å®ç° | AVX-512, NEON | CUDA, ROCm |

### 2.2 SIMT çš„æ ¸å¿ƒä¼˜åŠ¿

1. **ç¼–ç¨‹ç®€åŒ–**ï¼šå¼€å‘è€…ç¼–å†™æ ‡é‡ä»£ç ï¼Œç¡¬ä»¶è‡ªåŠ¨å‘é‡åŒ–æ‰§è¡Œ
2. **çµæ´»åˆ†æ”¯**ï¼šæ”¯æŒçº¿ç¨‹çº§æ¡ä»¶æ‰§è¡Œï¼Œæ— éœ€æ‰‹åŠ¨ç®¡ç†æ©ç 
3. **ç¡¬ä»¶æŠ½è±¡**ï¼šå‘é‡å®½åº¦å¯¹ç¨‹åºå‘˜é€æ˜ï¼Œä»£ç å¯è·¨ä»£è¿è¡Œ

> [!tip] SIMT æœ¬è´¨
> SIMT çš„æœ¬è´¨æ˜¯**å°† SIMD ç¡¬ä»¶æš´éœ²ä¸ºå¤šçº¿ç¨‹ç¼–ç¨‹æ¨¡å‹**ï¼Œè®©ç¨‹åºå‘˜ä»¥æ ‡é‡æ€ç»´ç¼–ç¨‹ï¼Œç”±ç¡¬ä»¶å®Œæˆå‘é‡åŒ–æ‰§è¡Œã€‚

---

## 3. GPU ç¼–ç¨‹æ¨¡å‹ï¼šGridã€Blockã€Thread å±‚æ¬¡ç»“æ„

### 3.1 ä¸‰å±‚æŠ½è±¡æ¨¡å‹

GPU ç¼–ç¨‹é‡‡ç”¨ä¸‰å±‚çº¿ç¨‹ç»„ç»‡ç»“æ„ï¼š

```mermaid
graph TB
    subgraph Grid["Grid (ç½‘æ ¼)"]
        subgraph Block1["Block (0,0)"]
            T1["Thread<br/>(0,0)"]
            T2["Thread<br/>(1,0)"]
            T3["..."]
        end
        subgraph Block2["Block (1,0)"]
            T4["Thread<br/>(0,0)"]
            T5["Thread<br/>(1,0)"]
            T6["..."]
        end
        subgraph Block3["Block (0,1)"]
            T7["..."]
        end
        subgraph Block4["..."]
            T8["..."]
        end
    end
    
    style Grid fill:#e1f5fe
    style Block1 fill:#fff3e0
    style Block2 fill:#fff3e0
    style Block3 fill:#fff3e0
    style Block4 fill:#fff3e0
```


| å±‚æ¬¡ | å®šä¹‰ | ç¡¬ä»¶æ˜ å°„ | èµ„æºå…±äº« |
|------|------|----------|----------|
| **Grid** | æ•´ä¸ªè®¡ç®—ä»»åŠ¡ | æ•´ä¸ª GPU | å…¨å±€å†…å­˜ |
| **Block** | åä½œçº¿ç¨‹ç»„ | å•ä¸ª SM | å…±äº«å†…å­˜ã€åŒæ­¥åŸè¯­ |
| **Thread** | æœ€å°æ‰§è¡Œå•å…ƒ | CUDA Core | ç§æœ‰å¯„å­˜å™¨ |
| **Warp** | è°ƒåº¦å•å…ƒï¼ˆ32çº¿ç¨‹ï¼‰ | Warp Scheduler | æŒ‡ä»¤æµ |

### 3.2 Kernel å¯åŠ¨é…ç½®

```cpp
// Kernel å¯åŠ¨è¯­æ³•
kernel<<<gridDim, blockDim, sharedMem, stream>>>(args...);

// ç¤ºä¾‹ï¼šå¤„ç† N ä¸ªå…ƒç´ 
int N = 1000000;
int blockSize = 256;
int gridSize = (N + blockSize - 1) / blockSize;
vectorAdd<<<gridSize, blockSize>>>(d_a, d_b, d_c, N);
```

### 3.3 çº¿ç¨‹ç´¢å¼•è®¡ç®—

```mermaid
graph LR
    subgraph "å…¨å±€çº¿ç¨‹ç´¢å¼•è®¡ç®—"
        A["blockIdx.x"] --> C["globalIdx"]
        B["blockDim.x"] --> C
        D["threadIdx.x"] --> C
    end
```

```cpp
// 1D ç´¢å¼•
int globalIdx = blockIdx.x * blockDim.x + threadIdx.x;

// 2D ç´¢å¼•
int row = blockIdx.y * blockDim.y + threadIdx.y;
int col = blockIdx.x * blockDim.x + threadIdx.x;
```

---

## 4. æ ¸å¿ƒæ¦‚å¿µå…³ç³»è¯¦è§£ï¼šKernelã€Gridã€Blockã€Threadã€Warpã€Stream

æœ¬èŠ‚è¯¦ç»†é˜è¿° GPU ç¼–ç¨‹ä¸­å„æ ¸å¿ƒæ¦‚å¿µä¹‹é—´çš„å…³ç³»ï¼Œè¿™æ˜¯ç†è§£ GPU æ¶æ„çš„å…³é”®ã€‚

### 4.1 æ¦‚å¿µå±‚æ¬¡æ€»è§ˆ

```mermaid
graph TD
    %% è½¯ä»¶é€»è¾‘å±‚
    subgraph Software_Level [è½¯ä»¶é€»è¾‘å±‚ / ç¨‹åºå‘˜è§†è§’]
        Stream[<b>Stream æµ</b><br/>ç®¡ç†ä»»åŠ¡æ’é˜Ÿå’Œå¼‚æ­¥æ‰§è¡Œ]
        Kernel[<b>Kernel ç®—å­</b><br/>ä¸€æ¬¡å…·ä½“çš„å‡½æ•°å¯åŠ¨]
        Grid[<b>Grid ç½‘æ ¼</b><br/>æ•´ä¸ªä»»åŠ¡çš„é€»è¾‘å¸ƒå±€]
        Block[<b>Block çº¿ç¨‹å—</b><br/>é€»è¾‘ä¸Šçš„åä½œå°ç»„]
        Thread[<b>Thread çº¿ç¨‹</b><br/>æœ€åŸºæœ¬çš„é€»è¾‘å•å…ƒ]
    end

    %% ç¡¬ä»¶æ‰§è¡Œå±‚
    subgraph Hardware_Level [ç¡¬ä»¶æ‰§è¡Œå±‚ / GPUå†…éƒ¨è§†è§’]
        GPU[<b>GPU æ˜¾å¡</b><br/>æ•´ä¸ªè®¡ç®—å·¥å‚]
        SM[<b>SM æµå¤šå¤„ç†å™¨</b><br/>ç‹¬ç«‹çš„è½¦é—´]
        Warp_Sched[<b>Warp Scheduler è°ƒåº¦å™¨</b><br/>æŒ‡æŒ¥ PC è·³åŠ¨, ç¿»ç‰Œå­]
        PC[<b>PC ç¨‹åºè®¡æ•°å™¨</b><br/>Warp çš„è¡ŒåŠ¨æŒ‡æŒ¥æ£’]
        Warp[<b>Warp çº¿ç¨‹æŸ</b><br/>32ä¸ªçº¿ç¨‹ç»„æˆçš„æœ€å°æ‰§è¡Œå•å…ƒ]
        Cores[<b>æ‰§è¡Œæ ¸å¿ƒ</b><br/>CUDA Cores / Tensor Cores]
    end

    %% æ•°æ®å±‚
    subgraph Data_Level [æ•°æ®å±‚]
        VRAM[(<b>VRAM æ˜¾å­˜</b>)]
        Tensor[<b>Tensor å¼ é‡</b><br/>å¤šç»´æ•°æ®çŸ©é˜µ]
        Reg[<b>Registers å¯„å­˜å™¨</b><br/>çº¿ç¨‹ç§æœ‰æ•°æ®åŒº]
    end

    %% å…³ç³»è¿æ¥
    Stream -- åŒ…å«ä¸€ç»„ --> Kernel
    Kernel -- è¡¨ç°ä¸º --> Grid
    Grid -- åˆ’åˆ†æˆå¤šä¸ª --> Block
    Block -- åŒ…å«å¤šä¸ª --> Thread

    GPU -- åŒ…å«å¤šä¸ª --> SM
    SM -- ç®¡ç†å¤šä¸ª --> Warp
    SM -- è°ƒåº¦ä¸­å¿ƒ --> Warp_Sched
    Warp_Sched -- æ§åˆ¶ --> PC
    PC -- å¹¿æ’­æŒ‡ä»¤ç»™ --> Warp
    Warp -- æ˜ å°„åˆ°ç¡¬ä»¶ --> Cores

    %% è½¯ç¡¬ä»¶æ˜ å°„å…³ç³»
    Block -. è¢«åˆ†å‘åˆ° .-> SM
    Thread -. ç»„åˆæˆ .-> Warp
    Thread -. é€šè¿‡ ID ç´¢å¼• .-> Tensor
    Tensor -. å­˜å‚¨åœ¨ .-> VRAM
    VRAM -. åŠ è½½æ•°æ®åˆ° .-> Reg
    Reg -. ä¾›æ ¸å¿ƒè®¡ç®— .-> Cores

    %% æ ·å¼ç¾åŒ–
    style Stream fill:#f9f,stroke:#333,stroke-width:2px
    style SM fill:#bbf,stroke:#333,stroke-width:2px
    style Warp fill:#dfd,stroke:#333,stroke-width:2px
    style Tensor fill:#ffd,stroke:#333,stroke-width:2px
    style PC fill:#f96,stroke:#333,stroke-width:2px
```

### 4.2 å„æ¦‚å¿µå®šä¹‰ä¸å…³ç³»

| æ¦‚å¿µ | å±‚æ¬¡ | å®šä¹‰ | ä¸å…¶ä»–æ¦‚å¿µçš„å…³ç³» |
|------|------|------|------------------|
| **Kernel** | è½¯ä»¶ | åœ¨ GPU ä¸Šæ‰§è¡Œçš„å¹¶è¡Œå‡½æ•° | å¯åŠ¨æ—¶åˆ›å»ºä¸€ä¸ª Grid |
| **Stream** | è½¯ä»¶ | GPU å‘½ä»¤çš„æ‰§è¡Œé˜Ÿåˆ— | å¯åŒ…å«å¤šä¸ª Kernelï¼Œæ§åˆ¶æ‰§è¡Œé¡ºåº |
| **Grid** | è½¯ä»¶ | Kernel çš„æ‰€æœ‰çº¿ç¨‹é›†åˆ | ç”±å¤šä¸ª Block ç»„æˆï¼Œå¯¹åº”ä¸€æ¬¡ Kernel è°ƒç”¨ |
| **Block** | è½¯ä»¶/ç¡¬ä»¶ | åä½œçº¿ç¨‹ç»„ | åŒ…å«å¤šä¸ª Threadï¼Œç»‘å®šåˆ°ä¸€ä¸ª SM æ‰§è¡Œ |
| **Thread** | è½¯ä»¶ | æœ€å°æ‰§è¡Œå•å…ƒ | 32 ä¸ª Thread ç»„æˆä¸€ä¸ª Warp |
| **Warp** | ç¡¬ä»¶ | è°ƒåº¦å’Œæ‰§è¡Œçš„åŸºæœ¬å•ä½ | SM ä»¥ Warp ä¸ºå•ä½è°ƒåº¦æ‰§è¡Œ |
| **SM** | ç¡¬ä»¶ | æµå¤šå¤„ç†å™¨ | æ‰§è¡Œåˆ†é…ç»™å®ƒçš„ Block |

> ä¸€ä¸ª Stream é‡ŒåŒ…å«äº†ä¸€ä¸² Kernelï¼ˆä¹Ÿå°±æ˜¯ Gridï¼‰ï¼Œè€Œä¸€ä¸ª Grid é‡ŒåŒ…å«äº†ä¸€å † Blockã€‚

### 4.3 ä» Kernel åˆ° Warpï¼šå®Œæ•´æ‰§è¡Œæµç¨‹

```mermaid
sequenceDiagram
    participant Host as Host (CPU)
    participant Driver as CUDA Driver
    participant TBS as Thread Block Scheduler
    participant SM as SM (æµå¤šå¤„ç†å™¨)
    participant WS as Warp Scheduler
    
    Host->>Driver: kernel<<<grid, block, 0, stream>>>(args)
    Driver->>Driver: åˆ›å»º Grid (gridDim.x Ã— gridDim.y Ã— gridDim.z ä¸ª Block)
    Driver->>TBS: æäº¤ Grid åˆ° GPU
    
    loop å¯¹æ¯ä¸ª Block
        TBS->>TBS: æ£€æŸ¥ SM èµ„æº (å¯„å­˜å™¨ã€å…±äº«å†…å­˜)
        TBS->>SM: åˆ†é… Block åˆ°ç©ºé—² SM
        SM->>SM: å°† Block çš„çº¿ç¨‹åˆ’åˆ†ä¸º Warp (æ¯ 32 ä¸ªçº¿ç¨‹)
        
        loop å¯¹æ¯ä¸ª Warp
            WS->>WS: é€‰æ‹©å°±ç»ªçš„ Warp
            WS->>SM: å‘å°„æŒ‡ä»¤æ‰§è¡Œ
        end
    end
    
    SM->>TBS: Block æ‰§è¡Œå®Œæˆ
    TBS->>Driver: Grid æ‰§è¡Œå®Œæˆ
    Driver->>Host: Kernel è¿”å›
```

å…·ä½“çš„è°ƒç”¨å…³ç³»å¯èƒ½æ˜¯ï¼š

```mermaid
flowchart TB
    A["ä½  (Python Code)"] -->|è°ƒç”¨| B["PyTorch (Framework)"]
    B -->|"kernel<<<...>>><br/>(è°ƒç”¨ CUDA Runtime API)"| C["CUDA Driver (.so/.dll)"]
    C -->|PCIe ä¼ è¾“æŒ‡ä»¤| D["GPU ç¡¬ä»¶ (Scheduler)"]
    
    B -.- B1["è¿™é‡Œè®¡ç®— Grid/Blockï¼Œå†³å®šç”¨å“ªä¸ª Stream"]
    C -.- C1["è¿™é‡ŒæŠŠä»»åŠ¡å¡è¿› Stream é˜Ÿåˆ—"]
    D -.- D1["è¿™é‡ŒçœŸæ­£æŠŠ Kernel æ”¾åˆ° SM ä¸Šè·‘"]
```

### 4.4 å…·ä½“ç¤ºä¾‹ï¼šå‘é‡åŠ æ³•çš„æ‰§è¡Œè¿‡ç¨‹

å‡è®¾æˆ‘ä»¬è¦å¯¹ 100,000 ä¸ªå…ƒç´ æ‰§è¡Œå‘é‡åŠ æ³•ï¼š

```cpp
// Kernel å®šä¹‰
__global__ void vectorAdd(float* A, float* B, float* C, int N) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < N) {
        C[idx] = A[idx] + B[idx];
    }
}

// Kernel å¯åŠ¨
int N = 100000;
int blockSize = 256;                              // æ¯ä¸ª Block 256 ä¸ªçº¿ç¨‹
int gridSize = (N + blockSize - 1) / blockSize;   // = 391 ä¸ª Block
vectorAdd<<<gridSize, blockSize>>>(d_A, d_B, d_C, N);
```

**æ‰§è¡Œåˆ†è§£**ï¼š

```mermaid
graph TB
    subgraph Kernel["vectorAdd Kernel"]
        subgraph Grid["Grid: 391 ä¸ª Block"]
            B0["Block 0<br/>256 threads<br/>(8 Warps)"]
            B1["Block 1<br/>256 threads<br/>(8 Warps)"]
            B2["Block 2<br/>256 threads<br/>(8 Warps)"]
            BN["...<br/>Block 390"]
        end
    end
    
    subgraph GPU["GPU ç¡¬ä»¶"]
        subgraph SM0["SM 0"]
            W0["Warp 0: Thread 0-31"]
            W1["Warp 1: Thread 32-63"]
            W2["..."]
            W7["Warp 7: Thread 224-255"]
        end
        subgraph SM1["SM 1"]
            W8["Block 1 çš„ Warps"]
        end
        subgraph SMN["SM N"]
            WN["..."]
        end
    end
    
    B0 -->|"åˆ†é…"| SM0
    B1 -->|"åˆ†é…"| SM1
    BN -->|"åˆ†é…"| SMN
```

| å±‚æ¬¡ | æ•°é‡ | è®¡ç®—æ–¹å¼ |
|------|------|----------|
| **Grid** | 1 | ä¸€æ¬¡ Kernel è°ƒç”¨ |
| **Block** | 391 | ceil(100000 / 256) |
| **Thread/Block** | 256 | ç¨‹åºå‘˜æŒ‡å®š |
| **Warp/Block** | 8 | 256 / 32 |
| **æ€» Thread** | 100,096 | 391 Ã— 256 (ç•¥å¤šäº N) |
| **æ€» Warp** | 3,128 | 391 Ã— 8 |

### 4.5 Stream ä¸ Kernel çš„å…³ç³»

**Stream** æ˜¯ç‹¬ç«‹äº Grid/Block/Thread å±‚æ¬¡çš„**æ‰§è¡Œé˜Ÿåˆ—**æ¦‚å¿µï¼š

```mermaid
graph TB
    subgraph Host["Host ç«¯"]
        S1["Stream 1"]
        S2["Stream 2"]
        S0["Stream 0 (Default)"]
    end
    
    subgraph S1_Ops["Stream 1 æ“ä½œé˜Ÿåˆ—"]
        S1_H2D["H2D Copy"]
        S1_K1["Kernel A"]
        S1_K2["Kernel B"]
        S1_D2H["D2H Copy"]
    end
    
    subgraph S2_Ops["Stream 2 æ“ä½œé˜Ÿåˆ—"]
        S2_H2D["H2D Copy"]
        S2_K1["Kernel C"]
        S2_D2H["D2H Copy"]
    end
    
    S1 --> S1_Ops
    S2 --> S2_Ops
    
    S1_H2D -->|"é¡ºåº"| S1_K1 -->|"é¡ºåº"| S1_K2 -->|"é¡ºåº"| S1_D2H
    S2_H2D -->|"é¡ºåº"| S2_K1 -->|"é¡ºåº"| S2_D2H
    
    S1_Ops -.->|"å¹¶è¡Œ"| S2_Ops
```

| ç‰¹æ€§ | Stream | Grid/Block/Thread |
|------|--------|-------------------|
| **ä½œç”¨** | æ§åˆ¶æ“ä½œçš„æ‰§è¡Œé¡ºåºå’Œå¹¶å‘ | ç»„ç»‡å¹¶è¡Œè®¡ç®—çš„çº¿ç¨‹ |
| **ç²’åº¦** | Kernel çº§åˆ« | çº¿ç¨‹çº§åˆ« |
| **å¹¶è¡Œæ–¹å¼** | ä¸åŒ Stream é—´å¹¶è¡Œ | åŒä¸€ Kernel å†…çº¿ç¨‹å¹¶è¡Œ |
| **åŒæ­¥** | cudaStreamSynchronize | __syncthreads() |
| **èµ„æº** | å‘½ä»¤é˜Ÿåˆ— | è®¡ç®—èµ„æº (SM, å¯„å­˜å™¨) |

### 4.6 Warp ä¸ Block/Thread çš„å…³ç³»

**Warp æ˜¯ç¡¬ä»¶æ¦‚å¿µï¼ŒBlock/Thread æ˜¯è½¯ä»¶æ¦‚å¿µ**ï¼š

```mermaid
graph TB
    subgraph Block["Block (blockDim = 128)"]
        subgraph Warp0["Warp 0"]
            T0["T0"] 
            T1["T1"]
            T31["...T31"]
        end
        subgraph Warp1["Warp 1"]
            T32["T32"]
            T33["T33"]
            T63["...T63"]
        end
        subgraph Warp2["Warp 2"]
            T64["T64"]
            T65["T65"]
            T95["...T95"]
        end
        subgraph Warp3["Warp 3"]
            T96["T96"]
            T97["T97"]
            T127["...T127"]
        end
    end
    
    style Warp0 fill:#ffcdd2
    style Warp1 fill:#c8e6c9
    style Warp2 fill:#bbdefb
    style Warp3 fill:#fff9c4
```

**Warp åˆ’åˆ†è§„åˆ™**ï¼š
- Thread æŒ‰ `threadIdx` é¡ºåºåˆ’åˆ†åˆ° Warp
- 1D Block: `Warp i` åŒ…å« `Thread [32i, 32i+31]`
- 2D/3D Block: å…ˆçº¿æ€§åŒ– threadIdxï¼Œå†æŒ‰ 32 åˆ’åˆ†

```cpp
// 2D Block çš„ Warp åˆ’åˆ†ç¤ºä¾‹
// blockDim = (16, 8) = 128 threads
// çº¿æ€§åŒ–: linearIdx = threadIdx.y * blockDim.x + threadIdx.x
// Warp 0: linearIdx 0-31   (y=0,x=0-15) + (y=1,x=0-15)
// Warp 1: linearIdx 32-63  (y=2,x=0-15) + (y=3,x=0-15)
// ...
```

### 4.7 å…³é”®ç†è§£è¦ç‚¹

:::tip[æ ¸å¿ƒå…³ç³»æ€»ç»“]
1. **Kernel â†’ Grid**: ä¸€æ¬¡ Kernel è°ƒç”¨åˆ›å»ºä¸€ä¸ª Grid
2. **Grid â†’ Block**: Grid ç”±å¤šä¸ª Block ç»„æˆï¼ŒBlock æ•°é‡ç”± `gridDim` å†³å®š
3. **Block â†’ Thread**: Block ç”±å¤šä¸ª Thread ç»„æˆï¼ŒThread æ•°é‡ç”± `blockDim` å†³å®š
4. **Thread â†’ Warp**: æ¯ 32 ä¸ªè¿ç»­ Thread è‡ªåŠ¨ç»„æˆä¸€ä¸ª Warpï¼ˆç¡¬ä»¶è¡Œä¸ºï¼‰
5. **Block â†’ SM**: æ¯ä¸ª Block è¢«åˆ†é…åˆ°ä¸€ä¸ª SM æ‰§è¡Œï¼ˆä¸å¯è·¨ SMï¼‰
6. **Warp â†’ æ‰§è¡Œ**: SM ä»¥ Warp ä¸ºå•ä½è°ƒåº¦å’Œæ‰§è¡ŒæŒ‡ä»¤
7. **Stream â†’ Kernel**: Stream æ§åˆ¶å¤šä¸ª Kernel çš„æ‰§è¡Œé¡ºåºå’Œå¹¶å‘
:::

:::warning[å¸¸è§è¯¯åŒº]
- **è¯¯åŒº 1**: Warp æ˜¯ç¨‹åºå‘˜åˆ›å»ºçš„ â†’ **é”™è¯¯**ï¼ŒWarp ç”±ç¡¬ä»¶è‡ªåŠ¨åˆ’åˆ†
- **è¯¯åŒº 2**: ä¸€ä¸ª Block åªèƒ½æœ‰ä¸€ä¸ª Warp â†’ **é”™è¯¯**ï¼ŒBlock å¯åŒ…å«å¤šä¸ª Warp
- **è¯¯åŒº 3**: Stream å½±å“ Block å†…çš„å¹¶è¡Œ â†’ **é”™è¯¯**ï¼ŒStream æ§åˆ¶ Kernel çº§å¹¶å‘
- **è¯¯åŒº 4**: Block å¯ä»¥è·¨ SM æ‰§è¡Œ â†’ **é”™è¯¯**ï¼ŒBlock ç»‘å®šåˆ°å•ä¸ª SM
:::

## 5. Warpï¼šGPU æ‰§è¡Œçš„åŸºæœ¬å•å…ƒ

### 5.1 Warp åŸºæœ¬æ¦‚å¿µ

**Warp** æ˜¯ GPU è°ƒåº¦å’Œæ‰§è¡Œçš„åŸºæœ¬å•ä½ï¼š

| å‚å•†     | åç§°        | çº¿ç¨‹æ•°                   | ç‰¹ç‚¹         |
| ------ | --------- | --------------------- | ---------- |
| NVIDIA | Warp      | 32                    | æ‰€æœ‰ CUDA æ¶æ„ |
| AMD    | Wavefront | 64 (CDNA) / 32 (RDNA) | æ¶æ„ç›¸å…³       |
| Intel  | EU Thread | 8-16                  | Xe æ¶æ„      |

### 5.2 Warp æ‰§è¡Œæœºåˆ¶

```mermaid
sequenceDiagram
    participant WS as Warp Scheduler
    participant RF as Register File
    participant ALU as Execution Units
    participant MEM as Memory System
    
    WS->>WS: é€‰æ‹©å°±ç»ª Warp
    WS->>RF: è¯»å–æ“ä½œæ•°
    RF->>ALU: å‘å°„æŒ‡ä»¤
    ALU->>ALU: 32 çº¿ç¨‹å¹¶è¡Œæ‰§è¡Œ
    ALU->>RF: å†™å›ç»“æœ
    
    Note over WS,MEM: è‹¥é‡å†…å­˜è®¿é—®
    ALU->>MEM: å‘èµ·å†…å­˜è¯·æ±‚
    WS->>WS: åˆ‡æ¢åˆ°å…¶ä»– Warp
    MEM->>RF: æ•°æ®è¿”å›
    WS->>WS: Warp é‡æ–°å°±ç»ª
```

### 5.3 åˆ†æ”¯åˆ†åŒ–ï¼ˆBranch Divergenceï¼‰

å½“ Warp å†…çº¿ç¨‹æ‰§è¡Œä¸åŒåˆ†æ”¯æ—¶ï¼Œå‘ç”Ÿ**åˆ†æ”¯åˆ†åŒ–**ï¼š

```mermaid
graph TB
    subgraph "åˆ†æ”¯åˆ†åŒ–ç¤ºä¾‹"
        A["if (threadIdx.x < 16)"]
        B["Path A: çº¿ç¨‹ 0-15"]
        C["Path B: çº¿ç¨‹ 16-31"]
        D["æ±‡åˆç‚¹"]
    end
    A --> B
    A --> C
    B --> D
    C --> D
    
    style B fill:#90EE90
    style C fill:#FFB6C1
```

| æ‰§è¡Œé˜¶æ®µ | æ´»è·ƒçº¿ç¨‹ | æ‰§è¡Œæ•ˆç‡ |
|----------|----------|----------|
| Path A | 0-15 (16/32) | 50% |
| Path B | 16-31 (16/32) | 50% |
| æ€»ä½“ | ä¸²è¡Œæ‰§è¡Œä¸¤è·¯å¾„ | 50% |

:::tip[ä¼˜åŒ–å»ºè®®]
- å°½é‡è®©åŒä¸€ Warp å†…çº¿ç¨‹æ‰§è¡Œç›¸åŒåˆ†æ”¯
- ä½¿ç”¨è°“è¯æ‰§è¡Œæ›¿ä»£çŸ­åˆ†æ”¯
- é‡ç»„æ•°æ®å¸ƒå±€å‡å°‘åˆ†åŒ–
:::


---

## 6. SMï¼ˆStreaming Multiprocessorï¼‰å¾®æ¶æ„

### 6.1 SM æ¶æ„æ¦‚è§ˆ

SM æ˜¯ GPU çš„æ ¸å¿ƒè®¡ç®—å•å…ƒï¼Œé›†æˆäº†è®¡ç®—ã€å­˜å‚¨ã€è°ƒåº¦ç­‰åŠŸèƒ½æ¨¡å—ï¼š

```mermaid
graph TB
    subgraph SM["Streaming Multiprocessor"]
        subgraph Scheduler["è°ƒåº¦å•å…ƒ"]
            WS1["Warp Scheduler 0"]
            WS2["Warp Scheduler 1"]
            WS3["Warp Scheduler 2"]
            WS4["Warp Scheduler 3"]
        end
        
        subgraph Compute["è®¡ç®—å•å…ƒ"]
            subgraph FP["FP32 Units"]
                CUDA1["CUDA Cores"]
            end
            subgraph INT["INT32 Units"]
                INT1["INT Cores"]
            end
            subgraph TC["Tensor Cores"]
                TC1["Tensor Core"]
            end
            subgraph SFU["ç‰¹æ®Šå‡½æ•°"]
                SFU1["SFU"]
            end
        end
        
        subgraph Memory["å­˜å‚¨å•å…ƒ"]
            RF["Register File<br/>64K x 32-bit"]
            SMEM["Shared Memory<br/>å¯é…ç½®"]
            L1["L1 Cache"]
            TEX["Texture Cache"]
        end
        
        subgraph Control["æ§åˆ¶å•å…ƒ"]
            IC["Instruction Cache"]
            CC["Constant Cache"]
        end
    end
    
    Scheduler --> Compute
    Compute --> Memory
    Control --> Scheduler
```

æˆ‘ä»¬æ‹†å¼€ GPU çš„ä¸€ä¸ª **SM (Streaming Multiprocessor)** çœ‹çœ‹ã€‚ä¸€ä¸ª SM å†…éƒ¨é€šå¸¸è¢«åˆ†æˆ 4 ä¸ª **Processing Blocks (Sub-Partitions)**ã€‚

æ¯ä¸ªå­å—é‡Œéƒ½æœ‰è‡ªå·±çš„ï¼š

- **Warp Scheduler (è°ƒåº¦å™¨)**ï¼šè´Ÿè´£ç›¯ç€å“ªäº› Warp å‡†å¤‡å¥½å¹²æ´»äº†ã€‚
- **Dispatch Unit (åˆ†å‘å•å…ƒ)**ï¼šè´Ÿè´£æŠŠæŒ‡ä»¤å‘å‡ºå»ã€‚
- **æ‰§è¡Œå•å…ƒ (Execution Units)**ï¼š
    - **CUDA Cores**ï¼šå¤„ç†åŸºç¡€æ•°å­¦è¿ç®—ï¼ˆåŠ å‡ä¹˜é™¤ã€æµ®ç‚¹ $FP32$ã€$INT32$ï¼‰ã€‚
    - **Tensor Cores**ï¼šä¸“é—¨å¤„ç†æ··åˆç²¾åº¦çš„çŸ©é˜µä¹˜æ³•ï¼ˆ$D = A \times B + C$ï¼‰ã€‚
    - **Special Function Units (SFU)**ï¼šå¤„ç†æ­£ä½™å¼¦ã€å¯¹æ•°ç­‰å¤æ‚å‡½æ•°ã€‚

### 6.2 SM æ¶æ„æ¼”è¿›

| æ¶æ„ | å¹´ä»½ | CUDA Cores/SM | Tensor Cores/SM | å…±äº«å†…å­˜ | å…³é”®ç‰¹æ€§ |
|------|------|---------------|-----------------|----------|----------|
| **Volta** | 2017 | 64 | 8 (ç¬¬1ä»£) | 96KB | é¦–æ¬¡å¼•å…¥ Tensor Core |
| **Turing** | 2018 | 64 | 8 (ç¬¬2ä»£) | 96KB | RT Core, INT8 æ¨ç† |
| **Ampere** | 2020 | 128 | 8 (ç¬¬3ä»£) | 164KB | TF32, ç¨€ç–åŒ–æ”¯æŒ |
| **Hopper** | 2022 | 128 | 8 (ç¬¬4ä»£) | 228KB | FP8, Transformer Engine |
| **Ada Lovelace** | 2022 | 128 | 8 (ç¬¬4ä»£) | 128KB | DLSS 3, AV1 ç¼–ç  |
| **Blackwell** | 2024 | 128 | 8 (ç¬¬5ä»£) | 256KB | FP4, ç¬¬äºŒä»£ Transformer Engine |

### 6.3 è®¡ç®—å•å…ƒè¯¦è§£

#### 6.3.1 CUDA Core vs Tensor Core

```mermaid
graph TB
    subgraph CUDA["CUDA Core"]
        C1["æ ‡é‡è¿ç®—å•å…ƒ"]
        C2["FP32/FP64/INT32"]
        C3["æ¯å‘¨æœŸ 1 FMA"]
        C4["é€šç”¨è®¡ç®—"]
    end
    
    subgraph Tensor["Tensor Core"]
        T1["çŸ©é˜µè¿ç®—å•å…ƒ"]
        T2["FP16/BF16/TF32/FP8/INT8"]
        T3["æ¯å‘¨æœŸ 4x4x4 MMA"]
        T4["AI åŠ é€Ÿ"]
    end
```

| ç‰¹æ€§ | CUDA Core | Tensor Core |
|------|-----------|-------------|
| **è®¡ç®—ç±»å‹** | æ ‡é‡ FMA | çŸ©é˜µ MMA (D = AÃ—B + C) |
| **ç²¾åº¦æ”¯æŒ** | FP64, FP32, INT32 | FP16, BF16, TF32, FP8, INT8, INT4 |
| **å•å‘¨æœŸåå** | 1 FMA | 64 FMA (4Ã—4Ã—4) |
| **å…¸å‹åº”ç”¨** | é€šç”¨è®¡ç®—ã€å›¾å½¢æ¸²æŸ“ | æ·±åº¦å­¦ä¹ è®­ç»ƒ/æ¨ç† |
| **ç¼–ç¨‹æ¥å£** | ç›´æ¥ä½¿ç”¨ | WMMA API / cuBLAS / cuDNN |


#### 6.3.2 Tensor Core å·¥ä½œåŸç†

Tensor Core æ‰§è¡Œ **æ··åˆç²¾åº¦çŸ©é˜µä¹˜ç´¯åŠ ï¼ˆMMAï¼‰** æ“ä½œï¼š

```
D[4Ã—4] = A[4Ã—4] Ã— B[4Ã—4] + C[4Ã—4]
```

```mermaid
graph LR
    subgraph Input["è¾“å…¥çŸ©é˜µ"]
        A["A (FP16)<br/>4Ã—4"]
        B["B (FP16)<br/>4Ã—4"]
        C["C (FP32)<br/>4Ã—4"]
    end
    
    subgraph TC["Tensor Core"]
        MMA["MMA è¿ç®—<br/>64 FMA/cycle"]
    end
    
    subgraph Output["è¾“å‡ºçŸ©é˜µ"]
        D["D (FP32)<br/>4Ã—4"]
    end
    
    A --> MMA
    B --> MMA
    C --> MMA
    MMA --> D
```

**å„ä»£ Tensor Core æ€§èƒ½å¯¹æ¯”**ï¼š

| GPU | æ¶æ„ | FP16 Tensor (TFLOPS) | FP8 Tensor (TFLOPS) | ç¨€ç–åŠ é€Ÿ |
|-----|------|---------------------|---------------------|----------|
| V100 | Volta | 125 | N/A | æ—  |
| A100 | Ampere | 312 | N/A | 2Ã— |
| H100 | Hopper | 989 | 1979 | 2Ã— |
| H200 | Hopper | 989 | 1979 (3958 sparse) | 2Ã— |
| B200 | Blackwell | 2250 | 4500 (9000 sparse) | 2Ã— |

### 6.4 SM èµ„æºåˆ†é…

æ¯ä¸ª SM çš„èµ„æºæ˜¯æœ‰é™çš„ï¼ŒBlock è°ƒåº¦æ—¶éœ€è¦è€ƒè™‘èµ„æºçº¦æŸï¼š

| èµ„æºç±»å‹ | Ampere (A100) | Hopper (H100) | å½±å“å› ç´  |
|----------|---------------|---------------|----------|
| æœ€å¤§çº¿ç¨‹æ•° | 2048 | 2048 | Block å¤§å° |
| æœ€å¤§ Block æ•° | 32 | 32 | Grid é…ç½® |
| æœ€å¤§ Warp æ•° | 64 | 64 | å ç”¨ç‡ |
| å¯„å­˜å™¨æ–‡ä»¶ | 64K Ã— 32-bit | 64K Ã— 32-bit | æ¯çº¿ç¨‹å¯„å­˜å™¨ä½¿ç”¨ |
| å…±äº«å†…å­˜ | 164 KB | 228 KB | æ¯ Block å…±äº«å†…å­˜ |

:::warning[å ç”¨ç‡è®¡ç®—]
SM å ç”¨ç‡ = å®é™…æ´»è·ƒ Warp æ•° / æœ€å¤§ Warp æ•°

å½±å“å ç”¨ç‡çš„å› ç´ ï¼š
- æ¯çº¿ç¨‹å¯„å­˜å™¨ä½¿ç”¨é‡
- æ¯ Block å…±äº«å†…å­˜ä½¿ç”¨é‡
- Block å¤§å°é…ç½®
:::


---

## 7. å†…å­˜å±‚æ¬¡ç»“æ„

### 7.1 GPU å†…å­˜å±‚æ¬¡æ¦‚è§ˆ

```mermaid
graph TB
    subgraph "å†…å­˜å±‚æ¬¡ (å»¶è¿Ÿé€’å¢, å®¹é‡é€’å¢)"
        R["å¯„å­˜å™¨ Register<br/>~1 cycle | 256KB/SM"]
        L1["L1 Cache / å…±äº«å†…å­˜<br/>~30 cycles | 128-228KB/SM"]
        L2["L2 Cache<br/>~200 cycles | 40-60MB"]
        HBM["HBM / GDDR<br/>~400 cycles | 24-192GB"]
    end
    
    R --> L1 --> L2 --> HBM
    
    style R fill:#90EE90
    style L1 fill:#87CEEB
    style L2 fill:#DDA0DD
    style HBM fill:#FFB6C1
```

### 7.2 å„çº§å†…å­˜ç‰¹æ€§å¯¹æ¯”

| å†…å­˜ç±»å‹ | ä½œç”¨åŸŸ | ç”Ÿå‘½å‘¨æœŸ | å»¶è¿Ÿ | å¸¦å®½ | å…¸å‹ç”¨é€” |
|----------|--------|----------|------|------|----------|
| **å¯„å­˜å™¨** | çº¿ç¨‹ç§æœ‰ | çº¿ç¨‹ | ~1 cycle | æœ€é«˜ | å±€éƒ¨å˜é‡ã€ä¸­é—´ç»“æœ |
| **å…±äº«å†…å­˜** | Block å†…å…±äº« | Block | ~30 cycles | é«˜ | çº¿ç¨‹åä½œã€æ•°æ®å¤ç”¨ |
| **L1 Cache** | SM ç§æœ‰ | è‡ªåŠ¨ç®¡ç† | ~30 cycles | é«˜ | è‡ªåŠ¨ç¼“å­˜ |
| **L2 Cache** | å…¨å±€å…±äº« | è‡ªåŠ¨ç®¡ç† | ~200 cycles | ä¸­ | è·¨ SM æ•°æ®å…±äº« |
| **å…¨å±€å†…å­˜** | å…¨å±€ | åº”ç”¨ | ~400 cycles | ä½ | å¤§è§„æ¨¡æ•°æ®å­˜å‚¨ |
| **å¸¸é‡å†…å­˜** | å…¨å±€åªè¯» | åº”ç”¨ | ~4 cycles (cached) | é«˜ (å¹¿æ’­) | å¸¸é‡å‚æ•° |
| **çº¹ç†å†…å­˜** | å…¨å±€åªè¯» | åº”ç”¨ | ~400 cycles | ä¸­ | 2D ç©ºé—´å±€éƒ¨æ€§æ•°æ® |


### 7.3 å†…å­˜åˆå¹¶è®¿é—®ï¼ˆMemory Coalescingï¼‰

**åˆå¹¶è®¿é—®**æ˜¯ GPU å†…å­˜ä¼˜åŒ–çš„å…³é”®ï¼š

```mermaid
graph TB
    subgraph Good["åˆå¹¶è®¿é—® âœ“"]
        G1["Thread 0 â†’ Addr 0"]
        G2["Thread 1 â†’ Addr 4"]
        G3["Thread 2 â†’ Addr 8"]
        G4["..."]
        G5["Thread 31 â†’ Addr 124"]
        G6["1 æ¬¡å†…å­˜äº‹åŠ¡"]
    end
    
    subgraph Bad["éåˆå¹¶è®¿é—® âœ—"]
        B1["Thread 0 â†’ Addr 0"]
        B2["Thread 1 â†’ Addr 128"]
        B3["Thread 2 â†’ Addr 256"]
        B4["..."]
        B5["Thread 31 â†’ Addr 3968"]
        B6["32 æ¬¡å†…å­˜äº‹åŠ¡"]
    end
    
    style Good fill:#90EE90
    style Bad fill:#FFB6C1
```

| è®¿é—®æ¨¡å¼ | å†…å­˜äº‹åŠ¡æ•° | å¸¦å®½åˆ©ç”¨ç‡ | ç¤ºä¾‹ |
|----------|------------|------------|------|
| å®Œå…¨åˆå¹¶ | 1 | 100% | `data[threadIdx.x]` |
| éƒ¨åˆ†åˆå¹¶ | 2-4 | 25-50% | `data[threadIdx.x * 2]` |
| å®Œå…¨åˆ†æ•£ | 32 | 3% | `data[random[threadIdx.x]]` |

### 7.4 å…±äº«å†…å­˜ Bank Conflict

å…±äº«å†…å­˜è¢«åˆ’åˆ†ä¸º 32 ä¸ª Bankï¼Œæ¯ä¸ª Bank å®½åº¦ä¸º 4 å­—èŠ‚ï¼š

```mermaid
graph LR
    subgraph Banks["å…±äº«å†…å­˜ Banks"]
        B0["Bank 0<br/>Addr 0,128,256..."]
        B1["Bank 1<br/>Addr 4,132,260..."]
        B2["Bank 2<br/>Addr 8,136,264..."]
        B31["Bank 31<br/>Addr 124,252,380..."]
    end
```

| è®¿é—®æ¨¡å¼ | Bank Conflict | æ€§èƒ½å½±å“ |
|----------|---------------|----------|
| æ¯çº¿ç¨‹è®¿é—®ä¸åŒ Bank | æ— å†²çª | æœ€ä¼˜ |
| å¤šçº¿ç¨‹è®¿é—®åŒä¸€ Bank ä¸åŒåœ°å€ | N-way conflict | ä¸²è¡ŒåŒ– N æ¬¡ |
| å¤šçº¿ç¨‹è®¿é—®åŒä¸€ Bank åŒä¸€åœ°å€ | å¹¿æ’­ | æ— æƒ©ç½š |


---

## 8. çº¿ç¨‹è°ƒåº¦æœºåˆ¶

### 8.1 åŒå±‚è°ƒåº¦æ¶æ„

GPU é‡‡ç”¨ä¸¤çº§è°ƒåº¦æœºåˆ¶å®ç°é«˜æ•ˆçš„çº¿ç¨‹ç®¡ç†ï¼š

```mermaid
graph TB
    subgraph Host["ä¸»æœºç«¯"]
        K["Kernel Launch"]
    end
    
    subgraph GPU["GPU"]
        subgraph GigaThread["GigaThread Engine"]
            TBS["Thread Block Scheduler<br/>çº¿ç¨‹å—è°ƒåº¦å™¨"]
        end
        
        subgraph SM1["SM 0"]
            WS1["Warp Scheduler"]
            W1["Warp 0-63"]
        end
        
        subgraph SM2["SM 1"]
            WS2["Warp Scheduler"]
            W2["Warp 0-63"]
        end
        
        subgraph SMN["SM N"]
            WSN["Warp Scheduler"]
            WN["Warp 0-63"]
        end
    end
    
    K --> TBS
    TBS --> SM1
    TBS --> SM2
    TBS --> SMN
```

### 8.2 Thread Block Scheduler

**èŒè´£**ï¼šå°† Grid ä¸­çš„ Block åˆ†é…åˆ°å¯ç”¨çš„ SM

```mermaid
sequenceDiagram
    participant K as Kernel
    participant TBS as Thread Block Scheduler
    participant SM0 as SM 0
    participant SM1 as SM 1
    
    K->>TBS: Launch Grid (64 Blocks)
    TBS->>TBS: æ£€æŸ¥ SM èµ„æº
    TBS->>SM0: åˆ†é… Block 0, 2, 4...
    TBS->>SM1: åˆ†é… Block 1, 3, 5...
    SM0->>TBS: Block 0 å®Œæˆ
    TBS->>SM0: åˆ†é… Block 32
    Note over TBS: åŠ¨æ€è´Ÿè½½å‡è¡¡
```

**åˆ†é…ç­–ç•¥**ï¼š
- Round-robin åˆå§‹åˆ†é…
- åŠ¨æ€è´Ÿè½½å‡è¡¡ï¼ˆBlock å®Œæˆååˆ†é…æ–° Blockï¼‰
- èµ„æºçº¦æŸæ£€æŸ¥ï¼ˆå¯„å­˜å™¨ã€å…±äº«å†…å­˜ï¼‰


### 8.3 Warp Scheduler

**èŒè´£**ï¼šåœ¨ SM å†…éƒ¨é€‰æ‹©å°±ç»ªçš„ Warp å‘å°„æ‰§è¡Œ

```mermaid
stateDiagram-v2
    [*] --> Ready: æŒ‡ä»¤å°±ç»ª
    Ready --> Issued: è¢«è°ƒåº¦å™¨é€‰ä¸­
    Issued --> Executing: æ‰§è¡Œä¸­
    Executing --> Ready: æŒ‡ä»¤å®Œæˆ
    Executing --> Stalled: ç­‰å¾…æ•°æ®
    Stalled --> Ready: æ•°æ®å°±ç»ª
    
    note right of Stalled: å†…å­˜è®¿é—®å»¶è¿Ÿ<br/>æŒ‡ä»¤ä¾èµ–<br/>åŒæ­¥ç­‰å¾…
```

**è°ƒåº¦å†³ç­–å› ç´ **ï¼š

| å› ç´  | è¯´æ˜ | å½±å“ |
|------|------|------|
| æŒ‡ä»¤å°±ç»ª | æ“ä½œæ•°æ˜¯å¦å¯ç”¨ | å¿…è¦æ¡ä»¶ |
| å†…å­˜å»¶è¿Ÿ | æ˜¯å¦ç­‰å¾…å†…å­˜è¿”å› | è·³è¿‡ç­‰å¾…ä¸­çš„ Warp |
| æŒ‡ä»¤ç±»å‹ | è®¡ç®—/å†…å­˜/ç‰¹æ®ŠæŒ‡ä»¤ | èµ„æºå¯ç”¨æ€§ |
| å…¬å¹³æ€§ | é¿å…é¥¥é¥¿ | è½®è¯¢ç­–ç•¥ |
| ä¼˜å…ˆçº§ | æŸäº› Warp ä¼˜å…ˆ | å¯é…ç½® |

### 8.4 å»¶è¿Ÿéšè—æœºåˆ¶

GPU é€šè¿‡**å¤§é‡å¹¶å‘ Warp** éšè—å†…å­˜è®¿é—®å»¶è¿Ÿï¼š

```mermaid
gantt
    title Warp è°ƒåº¦ä¸å»¶è¿Ÿéšè—
    dateFormat X
    axisFormat %s
    
    section Warp 0
    è®¡ç®—    :w0c1, 0, 2
    å†…å­˜ç­‰å¾… :w0m, 2, 10
    è®¡ç®—    :w0c2, 10, 12
    
    section Warp 1
    ç­‰å¾…    :w1w, 0, 2
    è®¡ç®—    :w1c1, 2, 4
    å†…å­˜ç­‰å¾… :w1m, 4, 12
    
    section Warp 2
    ç­‰å¾…    :w2w, 0, 4
    è®¡ç®—    :w2c1, 4, 6
    å†…å­˜ç­‰å¾… :w2m, 6, 14
    
    section Warp 3
    ç­‰å¾…    :w3w, 0, 6
    è®¡ç®—    :w3c1, 6, 8
    å†…å­˜ç­‰å¾… :w3m, 8, 16
```

**å»¶è¿Ÿéšè—å…¬å¼**ï¼š

```
æ‰€éœ€ Warp æ•° = å†…å­˜å»¶è¿Ÿ(cycles) / æŒ‡ä»¤åå(cycles/instruction)

ç¤ºä¾‹ï¼š
- å†…å­˜å»¶è¿Ÿï¼š400 cycles
- æŒ‡ä»¤ååï¼š4 cycles/instruction
- æ‰€éœ€ Warpï¼š400 / 4 = 100 Warps
```


---

## 9. Stream ä¸å¹¶å‘æ‰§è¡Œ

### 9.1 Stream æ¦‚å¿µ

**Stream** æ˜¯ GPU ä¸Šçš„å‘½ä»¤é˜Ÿåˆ—ï¼Œå®ç°å¼‚æ­¥æ‰§è¡Œå’Œå¹¶å‘ï¼š

```mermaid
graph TB
    subgraph Host["Host (CPU)"]
        H1["cudaMemcpyAsync"]
        H2["kernel<<<>>>"]
        H3["cudaMemcpyAsync"]
    end
    
    subgraph Streams["GPU Streams"]
        subgraph S0["Stream 0 (Default)"]
            S0_1["åŒæ­¥æ‰§è¡Œ"]
        end
        
        subgraph S1["Stream 1"]
            S1_1["H2D Copy"]
            S1_2["Kernel A"]
            S1_3["D2H Copy"]
        end
        
        subgraph S2["Stream 2"]
            S2_1["H2D Copy"]
            S2_2["Kernel B"]
            S2_3["D2H Copy"]
        end
    end
    
    H1 --> S1_1
    H2 --> S1_2
    H3 --> S1_3
```

### 9.2 Stream å¹¶å‘æ¨¡å¼

| æ¨¡å¼ | æè¿° | é€‚ç”¨åœºæ™¯ |
|------|------|----------|
| **é¡ºåºæ‰§è¡Œ** | å• Streamï¼Œæ“ä½œä¸²è¡Œ | ç®€å•åº”ç”¨ |
| **Compute-Copy é‡å ** | è®¡ç®—ä¸æ•°æ®ä¼ è¾“å¹¶è¡Œ | æ•°æ®å¯†é›†å‹ |
| **å¤š Kernel å¹¶å‘** | å¤šä¸ªå° Kernel å¹¶è¡Œ | èµ„æºæœªé¥±å’Œæ—¶ |
| **å¤š GPU å¹¶è¡Œ** | è·¨è®¾å¤‡å¹¶å‘ | å¤§è§„æ¨¡è®­ç»ƒ |

### 9.3 Compute-Copy é‡å ç¤ºä¾‹

```mermaid
gantt
    title Stream å¹¶å‘æ‰§è¡Œæ—¶é—´çº¿
    dateFormat X
    axisFormat %s
    
    section Stream 1
    H2D Copy 1  :s1h2d, 0, 3
    Kernel 1    :s1k, 3, 8
    D2H Copy 1  :s1d2h, 8, 11
    
    section Stream 2
    H2D Copy 2  :s2h2d, 1, 4
    Kernel 2    :s2k, 4, 9
    D2H Copy 2  :s2d2h, 9, 12
    
    section Stream 3
    H2D Copy 3  :s3h2d, 2, 5
    Kernel 3    :s3k, 5, 10
    D2H Copy 3  :s3d2h, 10, 13
```

```cpp
// å¤š Stream å¹¶å‘ç¤ºä¾‹
cudaStream_t streams[3];
for (int i = 0; i < 3; i++) {
    cudaStreamCreate(&streams[i]);
    cudaMemcpyAsync(d_in[i], h_in[i], size, 
                    cudaMemcpyHostToDevice, streams[i]);
    kernel<<<grid, block, 0, streams[i]>>>(d_in[i], d_out[i]);
    cudaMemcpyAsync(h_out[i], d_out[i], size, 
                    cudaMemcpyDeviceToHost, streams[i]);
}
```


---

## 10. è°“è¯æ‰§è¡Œä¸åˆ†æ”¯å¤„ç†

### 10.1 è°“è¯å¯„å­˜å™¨ï¼ˆPredicate Registersï¼‰

GPU ä½¿ç”¨**è°“è¯å¯„å­˜å™¨**å®ç°æ¡ä»¶æ‰§è¡Œï¼Œé¿å…åˆ†æ”¯å¼€é”€ï¼š

```mermaid
graph LR
    subgraph Traditional["ä¼ ç»Ÿåˆ†æ”¯"]
        T1["if (cond)"] --> T2["branch taken"]
        T1 --> T3["branch not taken"]
        T2 --> T4["merge"]
        T3 --> T4
    end
    
    subgraph Predicated["è°“è¯æ‰§è¡Œ"]
        P1["set predicate"] --> P2["exec if pred=1"]
        P1 --> P3["exec if pred=0"]
        P2 --> P4["both paths execute"]
        P3 --> P4
    end
```

### 10.2 è°“è¯æ‰§è¡Œæœºåˆ¶

| æ¶æ„ | è°“è¯å¯„å­˜å™¨ | ç”¨é€” |
|------|------------|------|
| NVIDIA CUDA | 7 ä¸ª 1-bit è°“è¯ | æ¡ä»¶æ‰§è¡Œã€å¾ªç¯æ§åˆ¶ |
| AMD RDNA | EXEC mask (64-bit) | Wavefront æ©ç  |
| Intel AVX-512 | k0-k7 (8 ä¸ª) | SIMD æ©ç æ“ä½œ |

### 10.3 PTX è°“è¯æŒ‡ä»¤ç¤ºä¾‹

```
// PTX è°“è¯æ‰§è¡Œ
setp.lt.s32 %p1, %r1, 10;     // è®¾ç½®è°“è¯: p1 = (r1 < 10)
@%p1 add.s32 %r2, %r2, 1;     // æ¡ä»¶æ‰§è¡Œ: if (p1) r2++
@!%p1 sub.s32 %r2, %r2, 1;    // æ¡ä»¶æ‰§è¡Œ: if (!p1) r2--
```

---

## 11. GPU æ€§èƒ½ä¼˜åŒ–è¦ç‚¹

### 11.1 ä¼˜åŒ–å±‚æ¬¡

```mermaid
graph TB
    subgraph "ä¼˜åŒ–å±‚æ¬¡ (å½±å“é€’å‡)"
        A1["ç®—æ³•ä¼˜åŒ–<br/>é€‰æ‹©åˆé€‚çš„å¹¶è¡Œç®—æ³•"]
        A2["å†…å­˜ä¼˜åŒ–<br/>åˆå¹¶è®¿é—®ã€å…±äº«å†…å­˜"]
        A3["æ‰§è¡Œä¼˜åŒ–<br/>å ç”¨ç‡ã€åˆ†æ”¯åˆ†åŒ–"]
        A4["æŒ‡ä»¤ä¼˜åŒ–<br/>æŒ‡ä»¤é€‰æ‹©ã€ILP"]
    end
    
    A1 --> A2 --> A3 --> A4
```


### 11.2 å…³é”®ä¼˜åŒ–ç­–ç•¥

| ä¼˜åŒ–æ–¹å‘ | ç­–ç•¥ | é¢„æœŸæ”¶ç›Š |
|----------|------|----------|
| **å†…å­˜å¸¦å®½** | åˆå¹¶è®¿é—®ã€å‘é‡åŒ–åŠ è½½ | 10-30Ã— |
| **å†…å­˜å¤ç”¨** | ä½¿ç”¨å…±äº«å†…å­˜ç¼“å­˜ | 5-20Ã— |
| **è®¡ç®—å¯†åº¦** | æé«˜ç®—æœ¯å¼ºåº¦ | 2-10Ã— |
| **å ç”¨ç‡** | è°ƒæ•´ Block å¤§å° | 1.5-3Ã— |
| **åˆ†æ”¯æ•ˆç‡** | å‡å°‘ Warp åˆ†åŒ– | 1.2-2Ã— |
| **æŒ‡ä»¤åå** | ä½¿ç”¨å¿«é€Ÿæ•°å­¦å‡½æ•° | 1.1-1.5Ã— |

### 11.3 æ€§èƒ½åˆ†æå·¥å…·

| å·¥å…· | å‚å•† | åŠŸèƒ½ |
|------|------|------|
| **Nsight Compute** | NVIDIA | Kernel çº§æ€§èƒ½åˆ†æ |
| **Nsight Systems** | NVIDIA | ç³»ç»Ÿçº§æ—¶é—´çº¿åˆ†æ |
| **rocprof** | AMD | ROCm æ€§èƒ½åˆ†æ |
| **Intel VTune** | Intel | è·¨å¹³å°æ€§èƒ½åˆ†æ |

---

## 12. ç°ä»£ GPU æ¶æ„å¯¹æ¯”

### 12.1 ä¸»æµæ•°æ®ä¸­å¿ƒ GPU å¯¹æ¯”

| è§„æ ¼ | H100 (Hopper) | H200 (Hopper) | MI300X (CDNA3) | B200 (Blackwell) |
|------|---------------|---------------|----------------|------------------|
| **SM/CU æ•°é‡** | 132 SM | 132 SM | 304 CU | 192 SM |
| **CUDA/Stream Cores** | 16,896 | 16,896 | 19,456 | 24,576 |
| **Tensor/Matrix Cores** | 528 | 528 | 1,216 | 768 |
| **æ˜¾å­˜** | 80GB HBM3 | 141GB HBM3e | 192GB HBM3 | 192GB HBM3e |
| **æ˜¾å­˜å¸¦å®½** | 3.35 TB/s | 4.8 TB/s | 5.3 TB/s | 8 TB/s |
| **FP16 Tensor** | 989 TFLOPS | 989 TFLOPS | 1,307 TFLOPS | 2,250 TFLOPS |
| **FP8 Tensor** | 1,979 TFLOPS | 1,979 TFLOPS | 2,614 TFLOPS | 4,500 TFLOPS |
| **TDP** | 700W | 700W | 750W | 1000W |


### 12.2 æ¶æ„ç‰¹æ€§å¯¹æ¯”

```mermaid
graph TB
    subgraph NVIDIA["NVIDIA Hopper/Blackwell"]
        N1["CUDA Core + Tensor Core"]
        N2["NVLink äº’è”"]
        N3["Transformer Engine"]
        N4["HBM3/HBM3e"]
    end
    
    subgraph AMD["AMD CDNA3"]
        A1["Stream Processor + Matrix Core"]
        A2["Infinity Fabric"]
        A3["AI Accelerator"]
        A4["HBM3"]
    end
    
    subgraph Intel["Intel Xe"]
        I1["EU + XMX"]
        I2["Xe Link"]
        I3["AMX æ”¯æŒ"]
        I4["HBM2e"]
    end
```

---

## 13. æ€»ç»“

### 13.1 GPU æ¶æ„æ ¸å¿ƒè¦ç‚¹

```mermaid
mindmap
  root((GPU æ¶æ„))
    æ‰§è¡Œæ¨¡å‹
      SIMT
      Warp/Wavefront
      åˆ†æ”¯åˆ†åŒ–
    è®¡ç®—å•å…ƒ
      CUDA Core
      Tensor Core
      SFU
    å†…å­˜å±‚æ¬¡
      å¯„å­˜å™¨
      å…±äº«å†…å­˜
      L1/L2 Cache
      HBM/GDDR
    è°ƒåº¦æœºåˆ¶
      Block Scheduler
      Warp Scheduler
      å»¶è¿Ÿéšè—
    å¹¶å‘æ‰§è¡Œ
      Stream
      Compute-Copy é‡å 
      å¤š GPU
```

### 13.2 å…³é”®è®¾è®¡åŸåˆ™

| åŸåˆ™ | å®ç°æ–¹å¼ | ç›®æ ‡ |
|------|----------|------|
| **ååé‡ä¼˜å…ˆ** | å¤§é‡ç®€å•æ ¸å¿ƒ | æœ€å¤§åŒ–å¹¶è¡Œåº¦ |
| **å»¶è¿Ÿéšè—** | ç¡¬ä»¶å¤šçº¿ç¨‹ | æ©ç›–å†…å­˜å»¶è¿Ÿ |
| **SIMT æ‰§è¡Œ** | Warp çº§è°ƒåº¦ | ç®€åŒ–ç¼–ç¨‹æ¨¡å‹ |
| **å†…å­˜å±‚æ¬¡** | å¤šçº§ç¼“å­˜ + å…±äº«å†…å­˜ | å‡å°‘å¸¦å®½å‹åŠ› |
| **ä¸“ç”¨åŠ é€Ÿ** | Tensor Core | AI è®¡ç®—åŠ é€Ÿ |

### 13.3 é€‚ç”¨åœºæ™¯

GPU æ¶æ„ç‰¹åˆ«é€‚åˆä»¥ä¸‹è®¡ç®—ä»»åŠ¡ï¼š

- **æ·±åº¦å­¦ä¹ **ï¼šçŸ©é˜µè¿ç®—å¯†é›†ï¼ŒTensor Core åŠ é€Ÿ
- **ç§‘å­¦è®¡ç®—**ï¼šå¤§è§„æ¨¡æ•°å€¼æ¨¡æ‹Ÿï¼Œé«˜ç²¾åº¦æµ®ç‚¹
- **å›¾å½¢æ¸²æŸ“**ï¼šå¹¶è¡Œå…‰æ …åŒ–ï¼Œçº¹ç†é‡‡æ ·
- **æ•°æ®åˆ†æ**ï¼šé«˜ååé‡æ•°æ®å¤„ç†
- **å¯†ç å­¦**ï¼šå¹¶è¡Œå“ˆå¸Œè®¡ç®—

---

## å‚è€ƒèµ„æ–™

1. NVIDIA CUDA Programming Guide
2. NVIDIA Hopper Architecture Whitepaper
3. AMD CDNA3 Architecture Whitepaper
4. Computer Architecture: A Quantitative Approach (Hennessy & Patterson)
5. Programming Massively Parallel Processors (Kirk & Hwu)

---

*æœ¬æ–‡æ¡£åŸºäºå…¬å¼€æŠ€æœ¯èµ„æ–™æ•´ç†ï¼Œå¦‚æœ‰ç–‘é—®æ¬¢è¿è®¨è®ºã€‚*