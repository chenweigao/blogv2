# Distributed Training

åˆ†å¸ƒå¼è®­ç»ƒæŠ€æœ¯å’Œå¤§è§„æ¨¡æ¨¡å‹è®­ç»ƒã€‚

## ğŸ“š ç°æœ‰æ–‡æ¡£

- [Megatron Parallel](./megatron_parallel.md) - Megatron å¹¶è¡Œç­–ç•¥
- [NCCL Test](./nccl-test.md) - NCCL é€šä¿¡æµ‹è¯•

## ğŸ”§ ä¸»é¢˜æ¦‚è§ˆ

### 1. å¹¶è¡Œç­–ç•¥

#### Data Parallelism
- DistributedDataParallel (DDP) - PyTorch æ•°æ®å¹¶è¡Œ
- Horovod - è·¨æ¡†æ¶åˆ†å¸ƒå¼è®­ç»ƒ
- Parameter Server - å‚æ•°æœåŠ¡å™¨æ¶æ„

#### Model Parallelism
- Pipeline Parallelism - æµæ°´çº¿å¹¶è¡Œ
- Tensor Parallelism - å¼ é‡å¹¶è¡Œ
- Sequence Parallelism - åºåˆ—å¹¶è¡Œ

#### Advanced Techniques
- 3D Parallelism - ä¸‰ç»´å¹¶è¡Œç­–ç•¥
- Expert Parallelism - ä¸“å®¶å¹¶è¡Œ (MoE)
- Gradient Compression - æ¢¯åº¦å‹ç¼©

### 2. å¤§æ¨¡å‹è®­ç»ƒ

#### Training Systems
- DeepSpeed - Microsoft åˆ†å¸ƒå¼è®­ç»ƒ
- FairScale - Facebook å¯æ‰©å±•è®­ç»ƒ
- Megatron-LM - NVIDIA å¤§æ¨¡å‹è®­ç»ƒ

#### Communication Optimization
- NCCL - NVIDIA é›†åˆé€šä¿¡åº“
- Gloo - Facebook é€šä¿¡åº“
- MPI - æ¶ˆæ¯ä¼ é€’æ¥å£

### 3. é›†ç¾¤ç®¡ç†

- Kubernetes for ML - K8s æœºå™¨å­¦ä¹ éƒ¨ç½²
- Slurm - ä½œä¸šè°ƒåº¦ç³»ç»Ÿ
- Ray - åˆ†å¸ƒå¼è®¡ç®—æ¡†æ¶
