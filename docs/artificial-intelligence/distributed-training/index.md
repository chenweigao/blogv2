# Distributed Training

åˆ†å¸ƒå¼è®­ç»ƒæŠ€æœ¯å’Œå¤§è§„æ¨¡æ¨¡å‹è®­ç»ƒæ–¹æ¡ˆã€‚

## ğŸ“š åˆ†å¸ƒå¼ç­–ç•¥

### Data Parallelism
- [DistributedDataParallel (DDP)](./ddp.md) - PyTorch æ•°æ®å¹¶è¡Œ
- [Horovod](./horovod.md) - è·¨æ¡†æ¶åˆ†å¸ƒå¼è®­ç»ƒ
- [Parameter Server](./parameter-server.md) - å‚æ•°æœåŠ¡å™¨æ¶æ„

### Model Parallelism
- [Pipeline Parallelism](./pipeline-parallel.md) - æµæ°´çº¿å¹¶è¡Œ
- [Tensor Parallelism](./tensor-parallel.md) - å¼ é‡å¹¶è¡Œ
- [Sequence Parallelism](./sequence-parallel.md) - åºåˆ—å¹¶è¡Œ

### Advanced Techniques
- [3D Parallelism](./3d-parallelism.md) - ä¸‰ç»´å¹¶è¡Œç­–ç•¥
- [Expert Parallelism](./expert-parallel.md) - ä¸“å®¶å¹¶è¡Œ (MoE)
- [Gradient Compression](./gradient-compression.md) - æ¢¯åº¦å‹ç¼©

## ğŸš€ å¤§æ¨¡å‹è®­ç»ƒ

### Training Systems
- [DeepSpeed](./deepspeed.md) - Microsoft åˆ†å¸ƒå¼è®­ç»ƒ
- [FairScale](./fairscale.md) - Facebook å¯æ‰©å±•è®­ç»ƒ
- [Megatron-LM](./megatron.md) - NVIDIA å¤§æ¨¡å‹è®­ç»ƒ

### Communication Optimization
- [NCCL](./nccl.md) - NVIDIA é›†åˆé€šä¿¡åº“
- [Gloo](./gloo.md) - Facebook é€šä¿¡åº“
- [MPI](./mpi.md) - æ¶ˆæ¯ä¼ é€’æ¥å£

## ğŸ”§ é›†ç¾¤ç®¡ç†

- [Kubernetes for ML](./k8s-ml.md) - K8s æœºå™¨å­¦ä¹ éƒ¨ç½²
- [Slurm](./slurm.md) - ä½œä¸šè°ƒåº¦ç³»ç»Ÿ
- [Ray](./ray.md) - åˆ†å¸ƒå¼è®¡ç®—æ¡†æ¶