# Distributed Training

åˆ†å¸ƒå¼è®­ç»ƒæŠ€æœ¯å’Œå¤§è§„æ¨¡æ¨¡å‹è®­ç»ƒæ–¹æ¡ˆã€‚

ä¸ºäº†æ»¡è¶³ LLM è®­ç»ƒçš„è®¡ç®—éœ€æ±‚ï¼Œå„ä¸ªå‚å•†å‡æ„å»ºäº†ç”±é«˜æ€§èƒ½ GPU ç»„æˆçš„å¤§è§„æ¨¡è®­ç»ƒé›†ç¾¤ï¼Œå¹¶é€šè¿‡é«˜å¸¦å®½é“¾è·¯äº’è¿ã€‚ä¸‹å›¾æè¿°äº†ç°ä»£é›†ç¾¤å¤§è§„æ¨¡è®­ç»ƒçš„é€šç”¨æŠ€æœ¯æ ˆï¼Œå¦‚å›¾æ‰€ç¤ºï¼ŒInfra å›¢é˜Ÿç®¡ç†æœ€æ ¸å¿ƒèµ„æºå¹¶æä¾›è®­ç»ƒä¼˜åŒ–ï¼Œåœ¨å…¶æ”¯æŒä¸‹ï¼Œç®—æ³•å›¢é˜Ÿä¸“æ³¨äºé€šè¿‡å„ä¸ªè®­ç»ƒæ–¹æ³•å°† LLM é€‚é…åˆ°é¢å‘ç”¨æˆ·çš„åº”ç”¨ç¨‹åºã€‚åœ¨é›†å›¢åœºæ™¯ä¸‹ï¼Œè®­ç»ƒé›†ç¾¤è¿˜æ”¯æŒå…¶ä»–æ·±åº¦å­¦ä¹ ä»»åŠ¡ï¼Œä¾‹å¦‚æ¨èæ¨¡å‹ï¼ˆæ·˜å¤©ï¼‰å’Œæ•°æ®ç”Ÿæˆæ¨¡å‹ï¼ˆDTï¼‰ã€‚

![](./images/index-1760498130374.png)

## 1. ğŸ“š åˆ†å¸ƒå¼ç­–ç•¥

### 1.1. Data Parallelism
- [DistributedDataParallel (DDP)](./ddp.md) - PyTorch æ•°æ®å¹¶è¡Œ
- [Horovod](./horovod.md) - è·¨æ¡†æ¶åˆ†å¸ƒå¼è®­ç»ƒ
- [Parameter Server](./parameter-server.md) - å‚æ•°æœåŠ¡å™¨æ¶æ„

### 1.2. Model Parallelism
- [Pipeline Parallelism](./pipeline-parallel.md) - æµæ°´çº¿å¹¶è¡Œ
- [Tensor Parallelism](./tensor-parallel.md) - å¼ é‡å¹¶è¡Œ
- [Sequence Parallelism](./sequence-parallel.md) - åºåˆ—å¹¶è¡Œ

### 1.3. Advanced Techniques
- [3D Parallelism](./3d-parallelism.md) - ä¸‰ç»´å¹¶è¡Œç­–ç•¥
- [Expert Parallelism](./expert-parallel.md) - ä¸“å®¶å¹¶è¡Œ (MoE)
- [Gradient Compression](./gradient-compression.md) - æ¢¯åº¦å‹ç¼©

## 2. ğŸš€ å¤§æ¨¡å‹è®­ç»ƒ

### 2.1. Training Systems
- [DeepSpeed](./deepspeed.md) - Microsoft åˆ†å¸ƒå¼è®­ç»ƒ
- [FairScale](./fairscale.md) - Facebook å¯æ‰©å±•è®­ç»ƒ
- [Megatron-LM](./megatron.md) - NVIDIA å¤§æ¨¡å‹è®­ç»ƒ

### 2.2. Communication Optimization
- [NCCL](./nccl.md) - NVIDIA é›†åˆé€šä¿¡åº“
- [Gloo](./gloo.md) - Facebook é€šä¿¡åº“
- [MPI](./mpi.md) - æ¶ˆæ¯ä¼ é€’æ¥å£

## 3. ğŸ”§ é›†ç¾¤ç®¡ç†

- [Kubernetes for ML](./k8s-ml.md) - K8s æœºå™¨å­¦ä¹ éƒ¨ç½²
- [Slurm](./slurm.md) - ä½œä¸šè°ƒåº¦ç³»ç»Ÿ
- [Ray](./ray.md) - åˆ†å¸ƒå¼è®¡ç®—æ¡†æ¶